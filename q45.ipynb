{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install wget\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bayesian-optimization in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (from bayesian-optimization) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (from bayesian-optimization) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (from bayesian-optimization) (2.0.2)\n",
      "Requirement already satisfied: colorama in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (from bayesian-optimization) (0.4.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget, os, gzip, pickle, random, re, sys\n",
    "\n",
    "IMDB_URL = 'http://dlvu.github.io/data/imdb.{}.pkl.gz'\n",
    "IMDB_FILE = 'imdb.{}.pkl.gz'\n",
    "\n",
    "PAD, START, END, UNK = '.pad', '.start', '.end', '.unk'\n",
    "\n",
    "def load_imdb(final=False, val=5000, seed=0, voc=None, char=False):\n",
    "\n",
    "    cst = 'char' if char else 'word'\n",
    "\n",
    "    imdb_url = IMDB_URL.format(cst)\n",
    "    imdb_file = IMDB_FILE.format(cst)\n",
    "\n",
    "    if not os.path.exists(imdb_file):\n",
    "        wget.download(imdb_url)\n",
    "\n",
    "    with gzip.open(imdb_file) as file:\n",
    "        sequences, labels, i2w, w2i = pickle.load(file)\n",
    "\n",
    "    if voc is not None and voc < len(i2w):\n",
    "        nw_sequences = {}\n",
    "\n",
    "        i2w = i2w[:voc]\n",
    "        w2i = {w: i for i, w in enumerate(i2w)}\n",
    "\n",
    "        mx, unk = voc, w2i['.unk']\n",
    "        for key, seqs in sequences.items():\n",
    "            nw_sequences[key] = []\n",
    "            for seq in seqs:\n",
    "                seq = [s if s < mx else unk for s in seq]\n",
    "                nw_sequences[key].append(seq)\n",
    "\n",
    "        sequences = nw_sequences\n",
    "\n",
    "    if final:\n",
    "        return (sequences['train'], labels['train']), (sequences['test'], labels['test']), (i2w, w2i), 2\n",
    "\n",
    "    # Make a validation split\n",
    "    random.seed(seed)\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    x_val, y_val = [], []\n",
    "\n",
    "    val_ind = set( random.sample(range(len(sequences['train'])), k=val) )\n",
    "    for i, (s, l) in enumerate(zip(sequences['train'], labels['train'])):\n",
    "        if i in val_ind:\n",
    "            x_val.append(s)\n",
    "            y_val.append(l)\n",
    "        else:\n",
    "            x_train.append(s)\n",
    "            y_train.append(l)\n",
    "\n",
    "    return (x_train, y_train), \\\n",
    "           (x_val, y_val), \\\n",
    "           (i2w, w2i), 2\n",
    "\n",
    "\n",
    "def gen_sentence(sent, g):\n",
    "\n",
    "    symb = '_[a-z]*'\n",
    "\n",
    "    while True:\n",
    "\n",
    "        match = re.search(symb, sent)\n",
    "        if match is None:\n",
    "            return sent\n",
    "\n",
    "        s = match.span()\n",
    "        sent = sent[:s[0]] + random.choice(g[sent[s[0]:s[1]]]) + sent[s[1]:]\n",
    "\n",
    "def gen_dyck(p):\n",
    "    open = 1\n",
    "    sent = '('\n",
    "    while open > 0:\n",
    "        if random.random() < p:\n",
    "            sent += '('\n",
    "            open += 1\n",
    "        else:\n",
    "            sent += ')'\n",
    "            open -= 1\n",
    "\n",
    "    return sent\n",
    "\n",
    "def gen_ndfa(p):\n",
    "\n",
    "    word = random.choice(['abc!', 'uvw!', 'klm!'])\n",
    "\n",
    "    s = ''\n",
    "    while True:\n",
    "        if random.random() < p:\n",
    "            return 's' + s + 's'\n",
    "        else:\n",
    "            s+= word\n",
    "\n",
    "def load_brackets(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='dyck')\n",
    "\n",
    "def load_ndfa(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='ndfa')\n",
    "\n",
    "def load_toy(n=50_000, char=True, seed=0, name='lang'):\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    if name == 'lang':\n",
    "        sent = '_s'\n",
    "\n",
    "        toy = {\n",
    "            '_s': ['_s _adv', '_np _vp', '_np _vp _prep _np', '_np _vp ( _prep _np )', '_np _vp _con _s' , '_np _vp ( _con _s )'],\n",
    "            '_adv': ['briefly', 'quickly', 'impatiently'],\n",
    "            '_np': ['a _noun', 'the _noun', 'a _adj _noun', 'the _adj _noun'],\n",
    "            '_prep': ['on', 'with', 'to'],\n",
    "            '_con' : ['while', 'but'],\n",
    "            '_noun': ['mouse', 'bunny', 'cat', 'dog', 'man', 'woman', 'person'],\n",
    "            '_vp': ['walked', 'walks', 'ran', 'runs', 'goes', 'went'],\n",
    "            '_adj': ['short', 'quick', 'busy', 'nice', 'gorgeous']\n",
    "        }\n",
    "\n",
    "        sentences = [ gen_sentence(sent, toy) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s : len(s))\n",
    "\n",
    "    elif name == 'dyck':\n",
    "\n",
    "        sentences = [gen_dyck(7./16.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    elif name == 'ndfa':\n",
    "\n",
    "        sentences = [gen_ndfa(1./4.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    else:\n",
    "        raise Exception(name)\n",
    "\n",
    "    tokens = set()\n",
    "    for s in sentences:\n",
    "\n",
    "        if char:\n",
    "            for c in s:\n",
    "                tokens.add(c)\n",
    "        else:\n",
    "            for w in s.split():\n",
    "                tokens.add(w)\n",
    "\n",
    "    i2t = [PAD, START, END, UNK] + list(tokens)\n",
    "    t2i = {t:i for i, t in enumerate(i2t)}\n",
    "\n",
    "    sequences = []\n",
    "    for s in sentences:\n",
    "        if char:\n",
    "            tok = list(s)\n",
    "        else:\n",
    "            tok = s.split()\n",
    "        sequences.append([t2i[t] for t in tok])\n",
    "\n",
    "    return sequences, (i2t, t2i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Classification: data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = load_imdb(final=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Classification, baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([14, 19, 9, 379, 22, 11, 50, 52, 53, 290], 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching data \n",
    "# sort data \n",
    "x_train_sorted = sorted(x_train, key = lambda s : len(s))\n",
    "# get index of sorted x_train\n",
    "sorted_index = [x_train.index(seq) for seq in x_train_sorted]\n",
    "# sort y_train using the indexes\n",
    "y_train_sorted = [y_train[i] for i in sorted_index]\n",
    "\n",
    "# batching \n",
    "x_batches = []\n",
    "y_batches = []\n",
    "# cut of value for batches -> batches are created with sequences that contain a max diff of 100\n",
    "batch_buffer = 100 \n",
    "# key for batching -? [index, current seq length]\n",
    "start = [0, len(x_train[0])] \n",
    "# batch\n",
    "for i, val in enumerate(x_train):\n",
    "    # if seq length is greater than batch_buffer create batch \n",
    "    if len(val) - start[1] > batch_buffer:\n",
    "        # create batch\n",
    "        x_batches.append(x_train[start[0] : i])\n",
    "        y_batches.append(y_train[start[0] : i])\n",
    "        # update index and current seq length\n",
    "        start[0] = i\n",
    "        start[1] = len(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "\n",
    "# padded batches \n",
    "px_batches = []\n",
    "unique = set()\n",
    "# apply padding per batch\n",
    "for batch in x_batches:\n",
    "    p_batch = [] # current patted batch\n",
    "    # get maximal seq length for current batch\n",
    "    max_size = max(len(seq) for seq in batch)\n",
    "    # loop over seq in batch\n",
    "    for seq in batch:\n",
    "        unique.update(seq)\n",
    "        # apply padding to seq and appedn\n",
    "        p_batch.append(seq + [0]*(max_size - len(seq)))\n",
    "    # append padded batch to padded batches\n",
    "    px_batches.append(p_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,1], [2,2], [3,3]])\n",
    "x[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elman(nn.Module):\n",
    "    \n",
    "    def __init__(self, insize=300, outsize=300, hsize=300):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(insize, hsize)\n",
    "        self.lin2 = nn.Linear(hsize, outsize) \n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # batch, len of sequence, embedding \n",
    "        b, t, e = x.size()\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(b, e, dtype=torch.float)\n",
    "        \n",
    "        prev_h = None\n",
    "        outs = []\n",
    "        # range over time \n",
    "        for i in range(t):\n",
    "            # inp = torch.cat([x[:, i, :], hidden], dim=1)\n",
    "            inp = []\n",
    "            # Compute first pass \n",
    "            xi = self.lin1(x[:, i , :])\n",
    "            \n",
    "            # manage hidden values \n",
    "            if prev_h is not None:\n",
    "                xh = xi \n",
    "            else:\n",
    "                xh = xi + prev_h\n",
    "\n",
    "            # update hidden states \n",
    "            xh = np.tanh(xh)\n",
    "            prev_h = xh\n",
    "\n",
    "            hidden = xh\n",
    "\n",
    "            # get outputs from sequence \n",
    "            out = self.lin2(xh)\n",
    "            outs.append(out[:, None, :])\n",
    "\n",
    "        return torch.cat(outs, dim=1), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elman(nn.Module):\n",
    "    \n",
    "    def __init__(self, insize=300, outsize=300, hsize=300):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin1 = nn.Linear(insize, hsize)\n",
    "        self.lin2 = nn.Linear(hsize, outsize) \n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # batch, len of sequence, embedding \n",
    "        b, t, e = x.size()\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(b, e, dtype=torch.float)\n",
    "        \n",
    "        outs = []\n",
    "        # range over time \n",
    "        for i in range(t):\n",
    "            inp = torch.cat([x[:, i, :], hidden], dim=1)\n",
    "\n",
    "            # Compute first pass \n",
    "            xi = self.lin1(inp)\n",
    "\n",
    "            # hidden \n",
    "            xh = np.tanh(xi)\n",
    "            hidden = xh\n",
    "\n",
    "            # get outputs from sequence \n",
    "            out = self.lin2(xh)\n",
    "\n",
    "            outs.append(out[:, None, :])\n",
    "\n",
    "        return torch.cat(outs, dim=1), hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman Network Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_batches_tens = [torch.tensor(i) for i in px_batches]\n",
    "y_batches_tens = [torch.tensor(i, dtype = torch.float32) for i in y_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2775, 110, 150])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(len(i2w), embedding_dim = 150)\n",
    "x_emb = emb(px_batches_tens[0])\n",
    "x_emb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2775, 110])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px_batches_tens[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  batch , lenght , embedding \n",
    "# embedding size = number of unique tokens in a batch = input size \n",
    "#  seq size = \n",
    "# embedding\n",
    "class ELMAN(nn.Module):\n",
    "    def __init__(self,embedding_size, hidden_size, output_size, dropout): \n",
    "                #  input_size, hidden_size, num_classes):\n",
    "        super(ELMAN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding(embedding_size, embedding_dim = 150)\n",
    "        self.rnn = nn.RNN(150, hidden_size, dropout = dropout, batch_first = True)\n",
    "        self.lin1 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h0):\n",
    "        # create emebeddings\n",
    "        x_emb = self.emb(x)\n",
    "\n",
    "        # pass through rnn\n",
    "        out, _ = self.rnn(x_emb, h0)\n",
    "\n",
    "        # predict\n",
    "        out = self.lin1(out[:, -1]) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs for network and hyperparameters\n",
    "embedding_size = len(i2w)\n",
    "hidden_size = 300\n",
    "output_size = 1\n",
    "alpha = 0.003\n",
    "epochs = 10 \n",
    "batch_size = len(px_batches_tens)\n",
    "\n",
    "def train_rnn(px_batches_tens, y_batches_tens, embedding_size, hidden_size, output_size, alpha, epochs, batch_size):\n",
    "    #inti network\n",
    "    rnn = ELMAN(embedding_size, hidden_size, output_size, dropout = 0)\n",
    "\n",
    "    # optimizers \n",
    "    obj_func = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), alpha)\n",
    "\n",
    "    e_loss = {\"loss\": [], \"norm_loss\": []}\n",
    "    for epoch in range(epochs):\n",
    "        batch_loss = 0.0\n",
    "        for idx, batch in enumerate(px_batches_tens):\n",
    "            h0 = torch.zeros(1, batch.shape[0], hidden_size) \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get network output \n",
    "            output = rnn(batch, h0)\n",
    "        \n",
    "            # get loss \n",
    "            loss = obj_func(output, y_batches_tens[idx])\n",
    "            \n",
    "            # update network\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update batch loss\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch}:\\nBatch loss: {batch_loss}, normalized loss: {batch_loss/batch_size}\")\n",
    "        # store loss\n",
    "        e_loss[\"loss\"].append(batch_loss)\n",
    "        e_loss[\"norm_loss\"].append(batch_loss/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rnn(px_batches_tens, y_batches_tens, embedding_size, hidden_size, output_size, alpha, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, dropout): \n",
    "                #  input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding(embedding_size, embedding_dim = 150)\n",
    "        self.rnn = nn.LSTM(150, hidden_size, dropout = dropout, batch_first=True)\n",
    "        self.lin1 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        # create emebeddings\n",
    "        x_emb = self.emb(x)\n",
    "\n",
    "        # pass through rnn\n",
    "        out, _ = self.rnn(x_emb, states)\n",
    "\n",
    "        # predict\n",
    "        out = self.lin1(out[:, -1]) \n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs for network and hyperparameters\n",
    "embedding_size = len(i2w)\n",
    "hidden_size = 300\n",
    "output_size = 1\n",
    "alpha = 0.003\n",
    "epochs = 10 \n",
    "num_layers = 1\n",
    "batch_size = len(px_batches_tens)\n",
    "\n",
    "#inti network\n",
    "lstm = LSTM(embedding_size, hidden_size, output_size, dropout = 0)\n",
    "\n",
    "# optimizers \n",
    "obj_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), alpha)\n",
    "\n",
    "\n",
    "e_loss = {\"loss\": [], \"norm_loss\": []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_loss = 0.0  \n",
    "    for idx, batch in enumerate(px_batches_tens):\n",
    "        # initialize hidden state and cell state \n",
    "        h0 = torch.zeros(1, batch.shape[0], hidden_size) \n",
    "        c0 = torch.zeros(1, batch.shape[0], hidden_size)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get network output \n",
    "        output = lstm(batch, (h0,c0))\n",
    "\n",
    "        # get loss \n",
    "        loss = obj_func(output, y_batches_tens[idx])\n",
    "        print(\"Loss of batch\",loss)\n",
    "        # update network\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update batch loss\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "    e_loss.append(batch_loss)\n",
    "    print(f\"Epoch {epoch}:\\nBatch loss: {batch_loss}, normalized loss: {batch_loss/batch_size}\")\n",
    "    # store loss\n",
    "    e_loss[\"loss\"].append(batch_loss)\n",
    "    e_loss[\"norm_loss\"].append(batch_loss/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, px_batches_tens, y_batches_tens, alpha, epochs, opt, model_setting = 0, hpt = True):\n",
    "\n",
    "    embedding_size = len(i2w)\n",
    "    hidden_size = 300\n",
    "    output_size = 1\n",
    "    num_layers = 1\n",
    "    batch_size = len(px_batches_tens)\n",
    "\n",
    "    # inti network\n",
    "    rnn = model(embedding_size, hidden_size, output_size, dropout = 0)\n",
    "\n",
    "    # set objective function \n",
    "    obj_func = nn.MSELoss()\n",
    "\n",
    "    # set optimizer \n",
    "    if opt == 0:\n",
    "        optimizer = torch.optim.Adam(rnn.parameters(), alpha)\n",
    "    elif opt == 1:\n",
    "        optimizer = torch.optim.Adadelta(rnn.parameters(), alpha)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(rnn.parameters(), alpha)\n",
    "    \n",
    "    # inti epoch loss\n",
    "    e_loss = {\"loss\": [], \"norm_loss\": []}\n",
    "    for epoch in range(int(epochs)):\n",
    "        # init batch loss\n",
    "        batch_loss = 0.0\n",
    "        for idx, batch in enumerate(px_batches_tens):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # settings for LSTM\n",
    "            if model_setting == 2:\n",
    "                h0 = torch.zeros(num_layers, batch.shape[0], hidden_size) \n",
    "                c0 = torch.zeros(1, batch.shape[0], hidden_size)\n",
    "\n",
    "                output = rnn(batch, (h0, c0))\n",
    "            # settings for ELMAN\n",
    "            elif model_setting == 1:\n",
    "                h0 = torch.zeros(num_layers, batch.shape[0], hidden_size)\n",
    "                \n",
    "                output = rnn(batch, h0)\n",
    "            # settings for MLP\n",
    "            else:\n",
    "                output = rnn(batch)\n",
    "          \n",
    "            # get loss \n",
    "            loss = obj_func(output, y_batches_tens[idx])\n",
    "            \n",
    "            # update network\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update batch loss\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        if not hpt: print(f\"Epoch {epoch}:\\nBatch loss: {batch_loss}, normalized loss: {batch_loss/batch_size}\")\n",
    "        # store loss\n",
    "        e_loss[\"loss\"].append(batch_loss)\n",
    "        e_loss[\"norm_loss\"].append(batch_loss/batch_size)\n",
    "    \n",
    "    return e_loss[\"norm_loss\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayes_opt\n",
    "\n",
    "#module for optimization\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction\n",
    "# module for logging data \n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "# module for retriving datat \n",
    "from bayes_opt.util import load_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP Hyperparameter tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqMLP import SeqMLP\n",
    "# parameter bounds\n",
    "pbounds = {\"alpha\" : ( 0.01, 0.003), \"epochs\": (100, 125), \"opt\": (-0.5, 2.5)}\n",
    "\n",
    "# define wrapped funciton\n",
    "def train_wrapper(alpha, epochs, opt):\n",
    "    opt = int(round(opt))\n",
    "    return train_rnn(SeqMLP, px_batches_tens, y_batches_tens, alpha, epochs, opt = opt, model_setting = 0)\n",
    "\n",
    "# create instance of optimizer \n",
    "optimizer_bayes = BayesianOptimization(\n",
    "    f = train_wrapper,\n",
    "    pbounds = pbounds,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "# create UtilityFunction object for aqu. function\n",
    "utility = UtilityFunction(kind = \"ei\", xi= 0.02)\n",
    "\n",
    "# set gaussian process parameter\n",
    "optimizer_bayes.set_gp_params(alpha = 1e-6)\n",
    "\n",
    "# create logger \n",
    "logger = JSONLogger(path = \"./tunning1.log\")\n",
    "optimizer_bayes.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# initial search \n",
    "optimizer_bayes.maximize(\n",
    "    init_points = 5, # number of random explorations before bayes_opt\n",
    "    n_iter = 15, # number of bayes_opt iterations\n",
    ")\n",
    "\n",
    "# print out the data from the initial run to check if bounds need update \n",
    "for i, param in enumerate(optimizer_bayes.res):\n",
    "    print(f\"Iteration {i}: \\n\\t {param}\")\n",
    "\n",
    "# get best parameter\n",
    "print(\"Best Parameters found: \")\n",
    "print(optimizer_bayes.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELMAN hyperparameter tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter bounds\n",
    "pbounds = {\"alpha\" : ( 0.01, 0.003), \"epochs\": (100, 125), \"opt\": (-0.5, 2.5)}\n",
    "\n",
    "# define wrapped funciton\n",
    "def train_wrapper(alpha, epochs, opt):\n",
    "    opt = int(round(opt))\n",
    "    return train_rnn(ELMAN, px_batches_tens, y_batches_tens, alpha, epochs, opt = opt, model_setting = 1)\n",
    "\n",
    "# create instance of optimizer \n",
    "optimizer_bayes = BayesianOptimization(\n",
    "    f = train_wrapper,\n",
    "    pbounds = pbounds,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "# create UtilityFunction object for aqu. function\n",
    "utility = UtilityFunction(kind = \"ei\", xi= 0.02)\n",
    "\n",
    "# set gaussian process parameter\n",
    "optimizer_bayes.set_gp_params(alpha = 1e-6)\n",
    "\n",
    "# create logger \n",
    "logger = JSONLogger(path = \"./tunning1.log\")\n",
    "optimizer_bayes.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# initial search \n",
    "optimizer_bayes.maximize(\n",
    "    init_points = 5, # number of random explorations before bayes_opt\n",
    "    n_iter = 15, # number of bayes_opt iterations\n",
    ")\n",
    "\n",
    "# print out the data from the initial run to check if bounds need update \n",
    "for i, param in enumerate(optimizer_bayes.res):\n",
    "    print(f\"Iteration {i}: \\n\\t {param}\")\n",
    "\n",
    "# get best parameter\n",
    "print(\"Best Parameters found: \")\n",
    "print(optimizer_bayes.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM hyperparameter tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter bounds\n",
    "pbounds = {\"alpha\" : ( 0.01, 0.003), \"epochs\": (100, 125), \"opt\": (-0.5, 2.5)}\n",
    "\n",
    "# define wrapped funciton\n",
    "def train_wrapper(alpha, epochs, opt):\n",
    "    opt = int(round(opt))\n",
    "    return train_rnn(LSTM, px_batches_tens, y_batches_tens, alpha, epochs, opt = opt, model_setting = 2)\n",
    "\n",
    "# create instance of optimizer \n",
    "optimizer_bayes = BayesianOptimization(\n",
    "    f = train_wrapper,\n",
    "    pbounds = pbounds,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "# create UtilityFunction object for aqu. function\n",
    "utility = UtilityFunction(kind = \"ei\", xi= 0.02)\n",
    "\n",
    "# set gaussian process parameter\n",
    "optimizer_bayes.set_gp_params(alpha = 1e-6)\n",
    "\n",
    "# create logger \n",
    "logger = JSONLogger(path = \"./tunning1.log\")\n",
    "optimizer_bayes.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# initial search \n",
    "optimizer_bayes.maximize(\n",
    "    init_points = 5, # number of random explorations before bayes_opt\n",
    "    n_iter = 15, # number of bayes_opt iterations\n",
    ")\n",
    "\n",
    "# print out the data from the initial run to check if bounds need update \n",
    "for i, param in enumerate(optimizer_bayes.res):\n",
    "    print(f\"Iteration {i}: \\n\\t {param}\")\n",
    "\n",
    "# get best parameter\n",
    "print(\"Best Parameters found: \")\n",
    "print(optimizer_bayes.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
