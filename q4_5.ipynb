{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install wget\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bayesian-optimization in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (from bayesian-optimization) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (from bayesian-optimization) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (from bayesian-optimization) (2.0.2)\n",
      "Requirement already satisfied: colorama in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (from bayesian-optimization) (0.4.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget, os, gzip, pickle, random, re, sys\n",
    "\n",
    "IMDB_URL = 'http://dlvu.github.io/data/imdb.{}.pkl.gz'\n",
    "IMDB_FILE = 'imdb.{}.pkl.gz'\n",
    "\n",
    "PAD, START, END, UNK = '.pad', '.start', '.end', '.unk'\n",
    "\n",
    "def load_imdb(final=False, val=5000, seed=0, voc=None, char=False):\n",
    "\n",
    "    cst = 'char' if char else 'word'\n",
    "\n",
    "    imdb_url = IMDB_URL.format(cst)\n",
    "    imdb_file = IMDB_FILE.format(cst)\n",
    "\n",
    "    if not os.path.exists(imdb_file):\n",
    "        wget.download(imdb_url)\n",
    "\n",
    "    with gzip.open(imdb_file) as file:\n",
    "        sequences, labels, i2w, w2i = pickle.load(file)\n",
    "\n",
    "    if voc is not None and voc < len(i2w):\n",
    "        nw_sequences = {}\n",
    "\n",
    "        i2w = i2w[:voc]\n",
    "        w2i = {w: i for i, w in enumerate(i2w)}\n",
    "\n",
    "        mx, unk = voc, w2i['.unk']\n",
    "        for key, seqs in sequences.items():\n",
    "            nw_sequences[key] = []\n",
    "            for seq in seqs:\n",
    "                seq = [s if s < mx else unk for s in seq]\n",
    "                nw_sequences[key].append(seq)\n",
    "\n",
    "        sequences = nw_sequences\n",
    "\n",
    "    if final:\n",
    "        return (sequences['train'], labels['train']), (sequences['test'], labels['test']), (i2w, w2i), 2\n",
    "\n",
    "    # Make a validation split\n",
    "    random.seed(seed)\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    x_val, y_val = [], []\n",
    "\n",
    "    val_ind = set( random.sample(range(len(sequences['train'])), k=val) )\n",
    "    for i, (s, l) in enumerate(zip(sequences['train'], labels['train'])):\n",
    "        if i in val_ind:\n",
    "            x_val.append(s)\n",
    "            y_val.append(l)\n",
    "        else:\n",
    "            x_train.append(s)\n",
    "            y_train.append(l)\n",
    "\n",
    "    return (x_train, y_train), \\\n",
    "           (x_val, y_val), \\\n",
    "           (i2w, w2i), 2\n",
    "\n",
    "\n",
    "def gen_sentence(sent, g):\n",
    "\n",
    "    symb = '_[a-z]*'\n",
    "\n",
    "    while True:\n",
    "\n",
    "        match = re.search(symb, sent)\n",
    "        if match is None:\n",
    "            return sent\n",
    "\n",
    "        s = match.span()\n",
    "        sent = sent[:s[0]] + random.choice(g[sent[s[0]:s[1]]]) + sent[s[1]:]\n",
    "\n",
    "def gen_dyck(p):\n",
    "    open = 1\n",
    "    sent = '('\n",
    "    while open > 0:\n",
    "        if random.random() < p:\n",
    "            sent += '('\n",
    "            open += 1\n",
    "        else:\n",
    "            sent += ')'\n",
    "            open -= 1\n",
    "\n",
    "    return sent\n",
    "\n",
    "def gen_ndfa(p):\n",
    "\n",
    "    word = random.choice(['abc!', 'uvw!', 'klm!'])\n",
    "\n",
    "    s = ''\n",
    "    while True:\n",
    "        if random.random() < p:\n",
    "            return 's' + s + 's'\n",
    "        else:\n",
    "            s+= word\n",
    "\n",
    "def load_brackets(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='dyck')\n",
    "\n",
    "def load_ndfa(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='ndfa')\n",
    "\n",
    "def load_toy(n=50_000, char=True, seed=0, name='lang'):\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    if name == 'lang':\n",
    "        sent = '_s'\n",
    "\n",
    "        toy = {\n",
    "            '_s': ['_s _adv', '_np _vp', '_np _vp _prep _np', '_np _vp ( _prep _np )', '_np _vp _con _s' , '_np _vp ( _con _s )'],\n",
    "            '_adv': ['briefly', 'quickly', 'impatiently'],\n",
    "            '_np': ['a _noun', 'the _noun', 'a _adj _noun', 'the _adj _noun'],\n",
    "            '_prep': ['on', 'with', 'to'],\n",
    "            '_con' : ['while', 'but'],\n",
    "            '_noun': ['mouse', 'bunny', 'cat', 'dog', 'man', 'woman', 'person'],\n",
    "            '_vp': ['walked', 'walks', 'ran', 'runs', 'goes', 'went'],\n",
    "            '_adj': ['short', 'quick', 'busy', 'nice', 'gorgeous']\n",
    "        }\n",
    "\n",
    "        sentences = [ gen_sentence(sent, toy) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s : len(s))\n",
    "\n",
    "    elif name == 'dyck':\n",
    "\n",
    "        sentences = [gen_dyck(7./16.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    elif name == 'ndfa':\n",
    "\n",
    "        sentences = [gen_ndfa(1./4.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    else:\n",
    "        raise Exception(name)\n",
    "\n",
    "    tokens = set()\n",
    "    for s in sentences:\n",
    "\n",
    "        if char:\n",
    "            for c in s:\n",
    "                tokens.add(c)\n",
    "        else:\n",
    "            for w in s.split():\n",
    "                tokens.add(w)\n",
    "\n",
    "    i2t = [PAD, START, END, UNK] + list(tokens)\n",
    "    t2i = {t:i for i, t in enumerate(i2t)}\n",
    "\n",
    "    sequences = []\n",
    "    for s in sentences:\n",
    "        if char:\n",
    "            tok = list(s)\n",
    "        else:\n",
    "            tok = s.split()\n",
    "        sequences.append([t2i[t] for t in tok])\n",
    "\n",
    "    return sequences, (i2t, t2i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Classification: data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = load_imdb(final=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.pad': 0,\n",
       " '.start': 1,\n",
       " '.end': 2,\n",
       " '.unk': 3,\n",
       " 'the': 4,\n",
       " 'and': 5,\n",
       " 'a': 6,\n",
       " 'of': 7,\n",
       " 'to': 8,\n",
       " 'is': 9,\n",
       " 'br': 10,\n",
       " 'it': 11,\n",
       " 'in': 12,\n",
       " 'i': 13,\n",
       " 'this': 14,\n",
       " 'that': 15,\n",
       " 's': 16,\n",
       " 'was': 17,\n",
       " 'as': 18,\n",
       " 'movie': 19,\n",
       " 'for': 20,\n",
       " 'with': 21,\n",
       " 'but': 22,\n",
       " 'film': 23,\n",
       " 'you': 24,\n",
       " 't': 25,\n",
       " 'on': 26,\n",
       " 'not': 27,\n",
       " 'he': 28,\n",
       " 'are': 29,\n",
       " 'his': 30,\n",
       " 'have': 31,\n",
       " 'one': 32,\n",
       " 'be': 33,\n",
       " 'all': 34,\n",
       " 'at': 35,\n",
       " 'they': 36,\n",
       " 'by': 37,\n",
       " 'an': 38,\n",
       " 'who': 39,\n",
       " 'so': 40,\n",
       " 'from': 41,\n",
       " 'like': 42,\n",
       " 'there': 43,\n",
       " 'or': 44,\n",
       " 'just': 45,\n",
       " 'her': 46,\n",
       " 'out': 47,\n",
       " 'about': 48,\n",
       " 'if': 49,\n",
       " 'has': 50,\n",
       " 'what': 51,\n",
       " 'some': 52,\n",
       " 'good': 53,\n",
       " 'can': 54,\n",
       " 'when': 55,\n",
       " 'more': 56,\n",
       " 'very': 57,\n",
       " 'she': 58,\n",
       " 'up': 59,\n",
       " 'no': 60,\n",
       " 'time': 61,\n",
       " 'my': 62,\n",
       " 'even': 63,\n",
       " 'would': 64,\n",
       " 'which': 65,\n",
       " 'only': 66,\n",
       " 'story': 67,\n",
       " 'really': 68,\n",
       " 'see': 69,\n",
       " 'their': 70,\n",
       " 'had': 71,\n",
       " 'me': 72,\n",
       " 'well': 73,\n",
       " 'we': 74,\n",
       " 'were': 75,\n",
       " 'than': 76,\n",
       " 'much': 77,\n",
       " 'bad': 78,\n",
       " 'get': 79,\n",
       " 'been': 80,\n",
       " 'other': 81,\n",
       " 'do': 82,\n",
       " 'people': 83,\n",
       " 'great': 84,\n",
       " 'will': 85,\n",
       " 'also': 86,\n",
       " 'into': 87,\n",
       " 'because': 88,\n",
       " 'how': 89,\n",
       " 'don': 90,\n",
       " 'him': 91,\n",
       " 'first': 92,\n",
       " 'most': 93,\n",
       " 'made': 94,\n",
       " 'its': 95,\n",
       " 'them': 96,\n",
       " 'then': 97,\n",
       " 'make': 98,\n",
       " 'way': 99,\n",
       " 'could': 100,\n",
       " 'too': 101,\n",
       " 'movies': 102,\n",
       " 'any': 103,\n",
       " 'after': 104,\n",
       " 'characters': 105,\n",
       " 'think': 106,\n",
       " 'watch': 107,\n",
       " 'character': 108,\n",
       " 'films': 109,\n",
       " 'two': 110,\n",
       " 'many': 111,\n",
       " 'seen': 112,\n",
       " 'being': 113,\n",
       " 'love': 114,\n",
       " 'plot': 115,\n",
       " 'never': 116,\n",
       " 'life': 117,\n",
       " 'acting': 118,\n",
       " 'where': 119,\n",
       " 'show': 120,\n",
       " 'did': 121,\n",
       " 'best': 122,\n",
       " 'know': 123,\n",
       " 'little': 124,\n",
       " 'over': 125,\n",
       " 'off': 126,\n",
       " 'ever': 127,\n",
       " 'man': 128,\n",
       " 'does': 129,\n",
       " 'your': 130,\n",
       " 'better': 131,\n",
       " 'end': 132,\n",
       " 'here': 133,\n",
       " 'scene': 134,\n",
       " 'still': 135,\n",
       " 'say': 136,\n",
       " 'these': 137,\n",
       " 'why': 138,\n",
       " 'scenes': 139,\n",
       " 'while': 140,\n",
       " 've': 141,\n",
       " 'm': 142,\n",
       " 'something': 143,\n",
       " 'such': 144,\n",
       " 'go': 145,\n",
       " 'should': 146,\n",
       " 'through': 147,\n",
       " 'back': 148,\n",
       " 'real': 149,\n",
       " 'those': 150,\n",
       " 'now': 151,\n",
       " 'thing': 152,\n",
       " 'watching': 153,\n",
       " 're': 154,\n",
       " 'actors': 155,\n",
       " 'doesn': 156,\n",
       " 'director': 157,\n",
       " 'didn': 158,\n",
       " 'years': 159,\n",
       " 'funny': 160,\n",
       " 'though': 161,\n",
       " 'old': 162,\n",
       " 'another': 163,\n",
       " 'work': 164,\n",
       " 'before': 165,\n",
       " 'actually': 166,\n",
       " 'nothing': 167,\n",
       " 'makes': 168,\n",
       " 'look': 169,\n",
       " 'find': 170,\n",
       " 'going': 171,\n",
       " 'new': 172,\n",
       " 'same': 173,\n",
       " 'lot': 174,\n",
       " 'every': 175,\n",
       " 'few': 176,\n",
       " 'again': 177,\n",
       " 'part': 178,\n",
       " 'world': 179,\n",
       " 'down': 180,\n",
       " 'cast': 181,\n",
       " 'us': 182,\n",
       " 'things': 183,\n",
       " 'want': 184,\n",
       " 'quite': 185,\n",
       " 'pretty': 186,\n",
       " 'horror': 187,\n",
       " 'around': 188,\n",
       " 'seems': 189,\n",
       " 'young': 190,\n",
       " 'take': 191,\n",
       " 'big': 192,\n",
       " 'however': 193,\n",
       " 'got': 194,\n",
       " 'thought': 195,\n",
       " 'fact': 196,\n",
       " 'enough': 197,\n",
       " 'long': 198,\n",
       " 'both': 199,\n",
       " 'give': 200,\n",
       " 'may': 201,\n",
       " 'own': 202,\n",
       " 'd': 203,\n",
       " 'between': 204,\n",
       " 'comedy': 205,\n",
       " 'series': 206,\n",
       " 'must': 207,\n",
       " 'right': 208,\n",
       " 'action': 209,\n",
       " 'music': 210,\n",
       " 'without': 211,\n",
       " 'guy': 212,\n",
       " 'times': 213,\n",
       " 'saw': 214,\n",
       " 'original': 215,\n",
       " 'always': 216,\n",
       " 'isn': 217,\n",
       " 'role': 218,\n",
       " 'come': 219,\n",
       " 'almost': 220,\n",
       " 'gets': 221,\n",
       " 'point': 222,\n",
       " 'interesting': 223,\n",
       " 'done': 224,\n",
       " 'whole': 225,\n",
       " 'least': 226,\n",
       " 'far': 227,\n",
       " 'bit': 228,\n",
       " 'script': 229,\n",
       " 'family': 230,\n",
       " 'minutes': 231,\n",
       " 'feel': 232,\n",
       " 'll': 233,\n",
       " 'am': 234,\n",
       " 'might': 235,\n",
       " 'making': 236,\n",
       " 'anything': 237,\n",
       " 'since': 238,\n",
       " 'tv': 239,\n",
       " 'last': 240,\n",
       " 'probably': 241,\n",
       " 'performance': 242,\n",
       " 'kind': 243,\n",
       " 'away': 244,\n",
       " 'girl': 245,\n",
       " 'yet': 246,\n",
       " 'fun': 247,\n",
       " 'anyone': 248,\n",
       " 'woman': 249,\n",
       " 'worst': 250,\n",
       " 'sure': 251,\n",
       " 'rather': 252,\n",
       " 'hard': 253,\n",
       " 'day': 254,\n",
       " 'each': 255,\n",
       " 'played': 256,\n",
       " 'found': 257,\n",
       " 'looking': 258,\n",
       " 'screen': 259,\n",
       " 'our': 260,\n",
       " 'although': 261,\n",
       " 'especially': 262,\n",
       " 'believe': 263,\n",
       " 'dvd': 264,\n",
       " 'having': 265,\n",
       " 'trying': 266,\n",
       " 'course': 267,\n",
       " 'everything': 268,\n",
       " 'set': 269,\n",
       " 'goes': 270,\n",
       " 'book': 271,\n",
       " 'ending': 272,\n",
       " 'put': 273,\n",
       " 'comes': 274,\n",
       " 'maybe': 275,\n",
       " 'place': 276,\n",
       " 'let': 277,\n",
       " 'shows': 278,\n",
       " 'three': 279,\n",
       " 'worth': 280,\n",
       " 'different': 281,\n",
       " 'actor': 282,\n",
       " 'once': 283,\n",
       " 'main': 284,\n",
       " 'someone': 285,\n",
       " 'sense': 286,\n",
       " 'american': 287,\n",
       " 'reason': 288,\n",
       " 'play': 289,\n",
       " 'effects': 290,\n",
       " 'looks': 291,\n",
       " 'true': 292,\n",
       " 'money': 293,\n",
       " 'watched': 294,\n",
       " 'wasn': 295,\n",
       " 'year': 296,\n",
       " 'everyone': 297,\n",
       " 'job': 298,\n",
       " 'together': 299,\n",
       " 'war': 300,\n",
       " 'high': 301,\n",
       " 'plays': 302,\n",
       " 'audience': 303,\n",
       " 'instead': 304,\n",
       " 'during': 305,\n",
       " 'half': 306,\n",
       " 'said': 307,\n",
       " 'later': 308,\n",
       " 'takes': 309,\n",
       " 'special': 310,\n",
       " 'john': 311,\n",
       " 'night': 312,\n",
       " 'seem': 313,\n",
       " 'beautiful': 314,\n",
       " 'left': 315,\n",
       " 'black': 316,\n",
       " 'himself': 317,\n",
       " 'seeing': 318,\n",
       " 'wife': 319,\n",
       " 'version': 320,\n",
       " 'shot': 321,\n",
       " 'excellent': 322,\n",
       " 'house': 323,\n",
       " 'idea': 324,\n",
       " 'star': 325,\n",
       " 'else': 326,\n",
       " 'mind': 327,\n",
       " 'death': 328,\n",
       " 'fan': 329,\n",
       " 'father': 330,\n",
       " 'used': 331,\n",
       " 'nice': 332,\n",
       " 'budget': 333,\n",
       " 'simply': 334,\n",
       " 'poor': 335,\n",
       " 'short': 336,\n",
       " 'completely': 337,\n",
       " 'second': 338,\n",
       " 'men': 339,\n",
       " 'read': 340,\n",
       " 'less': 341,\n",
       " 'along': 342,\n",
       " 'top': 343,\n",
       " 'home': 344,\n",
       " 'dead': 345,\n",
       " 'help': 346,\n",
       " 'line': 347,\n",
       " 'hollywood': 348,\n",
       " 'kids': 349,\n",
       " 'either': 350,\n",
       " 'boring': 351,\n",
       " 'friends': 352,\n",
       " 'camera': 353,\n",
       " 'production': 354,\n",
       " 'try': 355,\n",
       " 'wrong': 356,\n",
       " 'enjoy': 357,\n",
       " 'low': 358,\n",
       " 'classic': 359,\n",
       " 'use': 360,\n",
       " 'given': 361,\n",
       " 'women': 362,\n",
       " 'need': 363,\n",
       " 'full': 364,\n",
       " 'school': 365,\n",
       " 'stupid': 366,\n",
       " 'next': 367,\n",
       " 'until': 368,\n",
       " 'performances': 369,\n",
       " 'rest': 370,\n",
       " 'truly': 371,\n",
       " 'couple': 372,\n",
       " 'video': 373,\n",
       " 'awful': 374,\n",
       " 'sex': 375,\n",
       " 'start': 376,\n",
       " 'recommend': 377,\n",
       " 'tell': 378,\n",
       " 'terrible': 379,\n",
       " 'mean': 380,\n",
       " 'remember': 381,\n",
       " 'getting': 382,\n",
       " 'won': 383,\n",
       " 'came': 384,\n",
       " 'understand': 385,\n",
       " 'perhaps': 386,\n",
       " 'name': 387,\n",
       " 'moments': 388,\n",
       " 'face': 389,\n",
       " 'person': 390,\n",
       " 'keep': 391,\n",
       " 'itself': 392,\n",
       " 'human': 393,\n",
       " 'wonderful': 394,\n",
       " 'mother': 395,\n",
       " 'playing': 396,\n",
       " 'style': 397,\n",
       " 'episode': 398,\n",
       " 'small': 399,\n",
       " 'others': 400,\n",
       " 'boy': 401,\n",
       " 'perfect': 402,\n",
       " 'stars': 403,\n",
       " 'early': 404,\n",
       " 'doing': 405,\n",
       " 'head': 406,\n",
       " 'often': 407,\n",
       " 'written': 408,\n",
       " 'definitely': 409,\n",
       " 'lines': 410,\n",
       " 'children': 411,\n",
       " 'dialogue': 412,\n",
       " 'gives': 413,\n",
       " 'piece': 414,\n",
       " 'couldn': 415,\n",
       " 'went': 416,\n",
       " 'case': 417,\n",
       " 'finally': 418,\n",
       " 'yes': 419,\n",
       " 'title': 420,\n",
       " 'live': 421,\n",
       " 'absolutely': 422,\n",
       " 'laugh': 423,\n",
       " 'oh': 424,\n",
       " 'certainly': 425,\n",
       " 'friend': 426,\n",
       " 'lost': 427,\n",
       " 'liked': 428,\n",
       " 'become': 429,\n",
       " 'entertaining': 430,\n",
       " 'worse': 431,\n",
       " 'sort': 432,\n",
       " 'cinema': 433,\n",
       " 'picture': 434,\n",
       " 'loved': 435,\n",
       " 'called': 436,\n",
       " 'hope': 437,\n",
       " 'felt': 438,\n",
       " 'mr': 439,\n",
       " 'overall': 440,\n",
       " 'guys': 441,\n",
       " 'based': 442,\n",
       " 'entire': 443,\n",
       " 'several': 444,\n",
       " 'supposed': 445,\n",
       " 'drama': 446,\n",
       " 'sound': 447,\n",
       " 'problem': 448,\n",
       " 'white': 449,\n",
       " 'against': 450,\n",
       " 'waste': 451,\n",
       " 'beginning': 452,\n",
       " 'dark': 453,\n",
       " 'fans': 454,\n",
       " 'game': 455,\n",
       " 'totally': 456,\n",
       " 'care': 457,\n",
       " 'humor': 458,\n",
       " 'direction': 459,\n",
       " 'wanted': 460,\n",
       " 'under': 461,\n",
       " 'seemed': 462,\n",
       " 'lives': 463,\n",
       " 'evil': 464,\n",
       " 'lead': 465,\n",
       " 'despite': 466,\n",
       " 'guess': 467,\n",
       " 'final': 468,\n",
       " 'example': 469,\n",
       " 'already': 470,\n",
       " 'turn': 471,\n",
       " 'throughout': 472,\n",
       " 'becomes': 473,\n",
       " 'killer': 474,\n",
       " 'b': 475,\n",
       " 'unfortunately': 476,\n",
       " 'able': 477,\n",
       " 'son': 478,\n",
       " 'quality': 479,\n",
       " 'days': 480,\n",
       " 'history': 481,\n",
       " 'heart': 482,\n",
       " 'side': 483,\n",
       " 'fine': 484,\n",
       " 'michael': 485,\n",
       " 'flick': 486,\n",
       " 'wants': 487,\n",
       " 'writing': 488,\n",
       " 'horrible': 489,\n",
       " 'amazing': 490,\n",
       " 'run': 491,\n",
       " 'today': 492,\n",
       " 'art': 493,\n",
       " 'town': 494,\n",
       " 'act': 495,\n",
       " 'works': 496,\n",
       " 'close': 497,\n",
       " 'kill': 498,\n",
       " 'god': 499,\n",
       " 'child': 500,\n",
       " 'matter': 501,\n",
       " 'etc': 502,\n",
       " 'tries': 503,\n",
       " 'viewer': 504,\n",
       " 'past': 505,\n",
       " 'genre': 506,\n",
       " 'enjoyed': 507,\n",
       " 'turns': 508,\n",
       " 'brilliant': 509,\n",
       " 'behind': 510,\n",
       " 'gave': 511,\n",
       " 'car': 512,\n",
       " 'stuff': 513,\n",
       " 'eyes': 514,\n",
       " 'parts': 515,\n",
       " 'favorite': 516,\n",
       " 'girls': 517,\n",
       " 'directed': 518,\n",
       " 'hand': 519,\n",
       " 'kid': 520,\n",
       " 'late': 521,\n",
       " 'city': 522,\n",
       " 'hour': 523,\n",
       " 'expect': 524,\n",
       " 'soon': 525,\n",
       " 'actress': 526,\n",
       " 'themselves': 527,\n",
       " 'obviously': 528,\n",
       " 'sometimes': 529,\n",
       " 'killed': 530,\n",
       " 'thinking': 531,\n",
       " 'myself': 532,\n",
       " 'starts': 533,\n",
       " 'decent': 534,\n",
       " 'stop': 535,\n",
       " 'type': 536,\n",
       " 'self': 537,\n",
       " 'daughter': 538,\n",
       " 'highly': 539,\n",
       " 'group': 540,\n",
       " 'blood': 541,\n",
       " 'says': 542,\n",
       " 'voice': 543,\n",
       " 'anyway': 544,\n",
       " 'writer': 545,\n",
       " 'known': 546,\n",
       " 'took': 547,\n",
       " 'heard': 548,\n",
       " 'happens': 549,\n",
       " 'except': 550,\n",
       " 'fight': 551,\n",
       " 'feeling': 552,\n",
       " 'experience': 553,\n",
       " 'coming': 554,\n",
       " 'slow': 555,\n",
       " 'moment': 556,\n",
       " 'stories': 557,\n",
       " 'leave': 558,\n",
       " 'police': 559,\n",
       " 'told': 560,\n",
       " 'extremely': 561,\n",
       " 'score': 562,\n",
       " 'violence': 563,\n",
       " 'hero': 564,\n",
       " 'involved': 565,\n",
       " 'strong': 566,\n",
       " 'chance': 567,\n",
       " 'lack': 568,\n",
       " 'ok': 569,\n",
       " 'hit': 570,\n",
       " 'hilarious': 571,\n",
       " 'cannot': 572,\n",
       " 'roles': 573,\n",
       " 'wouldn': 574,\n",
       " 'happen': 575,\n",
       " 'wonder': 576,\n",
       " 'living': 577,\n",
       " 'particularly': 578,\n",
       " 'including': 579,\n",
       " 'save': 580,\n",
       " 'hell': 581,\n",
       " 'crap': 582,\n",
       " 'brother': 583,\n",
       " 'murder': 584,\n",
       " 'looked': 585,\n",
       " 'cool': 586,\n",
       " 'david': 587,\n",
       " 'simple': 588,\n",
       " 'please': 589,\n",
       " 'age': 590,\n",
       " 'cut': 591,\n",
       " 'obvious': 592,\n",
       " 'complete': 593,\n",
       " 'song': 594,\n",
       " 'gore': 595,\n",
       " 'happened': 596,\n",
       " 'serious': 597,\n",
       " 'james': 598,\n",
       " 'attempt': 599,\n",
       " 'ago': 600,\n",
       " 'taken': 601,\n",
       " 'shown': 602,\n",
       " 'robert': 603,\n",
       " 'husband': 604,\n",
       " 'english': 605,\n",
       " 'reality': 606,\n",
       " 'seriously': 607,\n",
       " 'released': 608,\n",
       " 'jokes': 609,\n",
       " 'opening': 610,\n",
       " 'interest': 611,\n",
       " 'across': 612,\n",
       " 'none': 613,\n",
       " 'alone': 614,\n",
       " 'sad': 615,\n",
       " 'possible': 616,\n",
       " 'exactly': 617,\n",
       " 'career': 618,\n",
       " 'hours': 619,\n",
       " 'number': 620,\n",
       " 'saying': 621,\n",
       " 'usually': 622,\n",
       " 'talent': 623,\n",
       " 'cinematography': 624,\n",
       " 'view': 625,\n",
       " 'documentary': 626,\n",
       " 'annoying': 627,\n",
       " 'relationship': 628,\n",
       " 'running': 629,\n",
       " 'yourself': 630,\n",
       " 'wish': 631,\n",
       " 'order': 632,\n",
       " 'huge': 633,\n",
       " 'shots': 634,\n",
       " 'whose': 635,\n",
       " 'body': 636,\n",
       " 'light': 637,\n",
       " 'taking': 638,\n",
       " 'country': 639,\n",
       " 'ridiculous': 640,\n",
       " 'power': 641,\n",
       " 'important': 642,\n",
       " 'middle': 643,\n",
       " 'call': 644,\n",
       " 'female': 645,\n",
       " 'level': 646,\n",
       " 'ends': 647,\n",
       " 'major': 648,\n",
       " 'started': 649,\n",
       " 'four': 650,\n",
       " 'word': 651,\n",
       " 'jack': 652,\n",
       " 'turned': 653,\n",
       " 'opinion': 654,\n",
       " 'scary': 655,\n",
       " 'change': 656,\n",
       " 'mostly': 657,\n",
       " 'usual': 658,\n",
       " 'beyond': 659,\n",
       " 'ones': 660,\n",
       " 'room': 661,\n",
       " 'happy': 662,\n",
       " 'silly': 663,\n",
       " 'rating': 664,\n",
       " 'somewhat': 665,\n",
       " 'words': 666,\n",
       " 'novel': 667,\n",
       " 'talking': 668,\n",
       " 'knows': 669,\n",
       " 'knew': 670,\n",
       " 'disappointed': 671,\n",
       " 'o': 672,\n",
       " 'strange': 673,\n",
       " 'apparently': 674,\n",
       " 'non': 675,\n",
       " 'modern': 676,\n",
       " 'attention': 677,\n",
       " 'upon': 678,\n",
       " 'single': 679,\n",
       " 'basically': 680,\n",
       " 'finds': 681,\n",
       " 'cheap': 682,\n",
       " 'due': 683,\n",
       " 'television': 684,\n",
       " 'musical': 685,\n",
       " 'earth': 686,\n",
       " 'problems': 687,\n",
       " 'miss': 688,\n",
       " 'clich': 689,\n",
       " 'episodes': 690,\n",
       " 'clearly': 691,\n",
       " 'local': 692,\n",
       " 'thriller': 693,\n",
       " 'king': 694,\n",
       " 'five': 695,\n",
       " 'british': 696,\n",
       " 'talk': 697,\n",
       " 'events': 698,\n",
       " 'class': 699,\n",
       " 'sequence': 700,\n",
       " 'aren': 701,\n",
       " 'team': 702,\n",
       " 'french': 703,\n",
       " 'moving': 704,\n",
       " 'fast': 705,\n",
       " 'ten': 706,\n",
       " 'review': 707,\n",
       " 'straight': 708,\n",
       " 'tells': 709,\n",
       " 'predictable': 710,\n",
       " 'die': 711,\n",
       " 'entertainment': 712,\n",
       " 'comic': 713,\n",
       " 'songs': 714,\n",
       " 'space': 715,\n",
       " 'whether': 716,\n",
       " 'george': 717,\n",
       " 'future': 718,\n",
       " 'dialog': 719,\n",
       " 'add': 720,\n",
       " 'above': 721,\n",
       " 'sets': 722,\n",
       " 'easily': 723,\n",
       " 'near': 724,\n",
       " 'enjoyable': 725,\n",
       " 'appears': 726,\n",
       " 'haven': 727,\n",
       " 'soundtrack': 728,\n",
       " 'hate': 729,\n",
       " 'bring': 730,\n",
       " 'romantic': 731,\n",
       " 'giving': 732,\n",
       " 'lots': 733,\n",
       " 'similar': 734,\n",
       " 'supporting': 735,\n",
       " 'message': 736,\n",
       " 'release': 737,\n",
       " 'eye': 738,\n",
       " 'mention': 739,\n",
       " 'within': 740,\n",
       " 'filmed': 741,\n",
       " 'sequel': 742,\n",
       " 'e': 743,\n",
       " 'falls': 744,\n",
       " 'sister': 745,\n",
       " 'rock': 746,\n",
       " 'monster': 747,\n",
       " 'clear': 748,\n",
       " 'needs': 749,\n",
       " 'dull': 750,\n",
       " 'suspense': 751,\n",
       " 'bunch': 752,\n",
       " 'surprised': 753,\n",
       " 'showing': 754,\n",
       " 'easy': 755,\n",
       " 'sorry': 756,\n",
       " 'tried': 757,\n",
       " 'certain': 758,\n",
       " 'working': 759,\n",
       " 'ways': 760,\n",
       " 'theme': 761,\n",
       " 'theater': 762,\n",
       " 'parents': 763,\n",
       " 'among': 764,\n",
       " 'named': 765,\n",
       " 'storyline': 766,\n",
       " 'lady': 767,\n",
       " 'stay': 768,\n",
       " 'fall': 769,\n",
       " 'effort': 770,\n",
       " 'stand': 771,\n",
       " 'minute': 772,\n",
       " 'gone': 773,\n",
       " 'feature': 774,\n",
       " 'dr': 775,\n",
       " 'buy': 776,\n",
       " 'dog': 777,\n",
       " 'tale': 778,\n",
       " 'using': 779,\n",
       " 'typical': 780,\n",
       " 'comments': 781,\n",
       " 'th': 782,\n",
       " 'mystery': 783,\n",
       " 'editing': 784,\n",
       " 'avoid': 785,\n",
       " 'subject': 786,\n",
       " 'deal': 787,\n",
       " 'oscar': 788,\n",
       " 'doubt': 789,\n",
       " 'richard': 790,\n",
       " 'okay': 791,\n",
       " 'peter': 792,\n",
       " 'fantastic': 793,\n",
       " 'viewers': 794,\n",
       " 'feels': 795,\n",
       " 'realistic': 796,\n",
       " 'nearly': 797,\n",
       " 'kept': 798,\n",
       " 'general': 799,\n",
       " 'check': 800,\n",
       " 'viewing': 801,\n",
       " 'elements': 802,\n",
       " 'points': 803,\n",
       " 'herself': 804,\n",
       " 'greatest': 805,\n",
       " 'means': 806,\n",
       " 'famous': 807,\n",
       " 'crime': 808,\n",
       " 'rent': 809,\n",
       " 'imagine': 810,\n",
       " 'paul': 811,\n",
       " 'red': 812,\n",
       " 'form': 813,\n",
       " 'actual': 814,\n",
       " 'follow': 815,\n",
       " 'period': 816,\n",
       " 'material': 817,\n",
       " 'believable': 818,\n",
       " 'tom': 819,\n",
       " 'move': 820,\n",
       " 'brought': 821,\n",
       " 'forget': 822,\n",
       " 'somehow': 823,\n",
       " 'begins': 824,\n",
       " 'animation': 825,\n",
       " 'leads': 826,\n",
       " 'reviews': 827,\n",
       " 'doctor': 828,\n",
       " 'imdb': 829,\n",
       " 'surprise': 830,\n",
       " 'figure': 831,\n",
       " 'america': 832,\n",
       " 'weak': 833,\n",
       " 'hear': 834,\n",
       " 'sit': 835,\n",
       " 'average': 836,\n",
       " 'open': 837,\n",
       " 'york': 838,\n",
       " 'deep': 839,\n",
       " 'fi': 840,\n",
       " 'sequences': 841,\n",
       " 'killing': 842,\n",
       " 'atmosphere': 843,\n",
       " 'wait': 844,\n",
       " 'learn': 845,\n",
       " 'eventually': 846,\n",
       " 'premise': 847,\n",
       " 'sci': 848,\n",
       " 'whatever': 849,\n",
       " 'expected': 850,\n",
       " 'dance': 851,\n",
       " 'lame': 852,\n",
       " 'boys': 853,\n",
       " 'indeed': 854,\n",
       " 'poorly': 855,\n",
       " 'particular': 856,\n",
       " 'note': 857,\n",
       " 'box': 858,\n",
       " 'situation': 859,\n",
       " 'truth': 860,\n",
       " 'hot': 861,\n",
       " 'shame': 862,\n",
       " 'third': 863,\n",
       " 'society': 864,\n",
       " 'season': 865,\n",
       " 'free': 866,\n",
       " 'decided': 867,\n",
       " 'difficult': 868,\n",
       " 'romance': 869,\n",
       " 'lee': 870,\n",
       " 'needed': 871,\n",
       " 'acted': 872,\n",
       " 'crew': 873,\n",
       " 'gay': 874,\n",
       " 'leaves': 875,\n",
       " 'sexual': 876,\n",
       " 'western': 877,\n",
       " 'emotional': 878,\n",
       " 'unless': 879,\n",
       " 'possibly': 880,\n",
       " 'footage': 881,\n",
       " 'de': 882,\n",
       " 'credits': 883,\n",
       " 'write': 884,\n",
       " 'forced': 885,\n",
       " 'memorable': 886,\n",
       " 'became': 887,\n",
       " 'reading': 888,\n",
       " 'begin': 889,\n",
       " 'air': 890,\n",
       " 'meet': 891,\n",
       " 'otherwise': 892,\n",
       " 'question': 893,\n",
       " 'male': 894,\n",
       " 'street': 895,\n",
       " 'nature': 896,\n",
       " 'meets': 897,\n",
       " 'beauty': 898,\n",
       " 'plus': 899,\n",
       " 'hands': 900,\n",
       " 'cheesy': 901,\n",
       " 'screenplay': 902,\n",
       " 'masterpiece': 903,\n",
       " 'superb': 904,\n",
       " 'effect': 905,\n",
       " 'features': 906,\n",
       " 'stage': 907,\n",
       " 'interested': 908,\n",
       " 'island': 909,\n",
       " 'laughs': 910,\n",
       " 'whom': 911,\n",
       " 'perfectly': 912,\n",
       " 'comment': 913,\n",
       " 'forward': 914,\n",
       " 'cop': 915,\n",
       " 'weird': 916,\n",
       " 'badly': 917,\n",
       " 'nor': 918,\n",
       " 'inside': 919,\n",
       " 'sounds': 920,\n",
       " 'previous': 921,\n",
       " 'japanese': 922,\n",
       " 'personal': 923,\n",
       " 'quickly': 924,\n",
       " 'total': 925,\n",
       " 'crazy': 926,\n",
       " 'baby': 927,\n",
       " 'mark': 928,\n",
       " 'keeps': 929,\n",
       " 'result': 930,\n",
       " 'girlfriend': 931,\n",
       " 'disney': 932,\n",
       " 'towards': 933,\n",
       " 'writers': 934,\n",
       " 'battle': 935,\n",
       " 'fire': 936,\n",
       " 'joe': 937,\n",
       " 'setting': 938,\n",
       " 'worked': 939,\n",
       " 'background': 940,\n",
       " 'incredibly': 941,\n",
       " 'earlier': 942,\n",
       " 'mess': 943,\n",
       " 'copy': 944,\n",
       " 'dumb': 945,\n",
       " 'business': 946,\n",
       " 'bill': 947,\n",
       " 'unique': 948,\n",
       " 'directors': 949,\n",
       " 'realize': 950,\n",
       " 'powerful': 951,\n",
       " 'water': 952,\n",
       " 'dramatic': 953,\n",
       " 'c': 954,\n",
       " 'rate': 955,\n",
       " 'older': 956,\n",
       " 'joke': 957,\n",
       " 'dream': 958,\n",
       " 'following': 959,\n",
       " 'pay': 960,\n",
       " 'plenty': 961,\n",
       " 'directing': 962,\n",
       " 'ask': 963,\n",
       " 'various': 964,\n",
       " 'creepy': 965,\n",
       " 'development': 966,\n",
       " 'appear': 967,\n",
       " 'front': 968,\n",
       " 'brings': 969,\n",
       " 'rich': 970,\n",
       " 'political': 971,\n",
       " 'apart': 972,\n",
       " 'admit': 973,\n",
       " 'cover': 974,\n",
       " 'leading': 975,\n",
       " 'fairly': 976,\n",
       " 'reasons': 977,\n",
       " 'spent': 978,\n",
       " 'portrayed': 979,\n",
       " 'era': 980,\n",
       " 'gun': 981,\n",
       " 'outside': 982,\n",
       " 'telling': 983,\n",
       " 'present': 984,\n",
       " 'party': 985,\n",
       " 'wasted': 986,\n",
       " 'fighting': 987,\n",
       " 'zombie': 988,\n",
       " 'william': 989,\n",
       " 'fails': 990,\n",
       " 'deserves': 991,\n",
       " 'success': 992,\n",
       " 'break': 993,\n",
       " 'twist': 994,\n",
       " 'list': 995,\n",
       " 'meant': 996,\n",
       " 'secret': 997,\n",
       " 'return': 998,\n",
       " 'create': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Classification, baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([14, 19, 9, 379, 22, 11, 50, 52, 53, 290], 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching data \n",
    "# sort data \n",
    "x_train_sorted = sorted(x_train, key = lambda s : len(s))\n",
    "# get index of sorted x_train\n",
    "sorted_index = [x_train.index(seq) for seq in x_train_sorted]\n",
    "# sort y_train using the indexes\n",
    "y_train_sorted = [y_train[i] for i in sorted_index]\n",
    "\n",
    "# batching \n",
    "x_batches = []\n",
    "y_batches = []\n",
    "# cut of value for batches -> batches are created with sequences that contain a max diff of 100\n",
    "batch_buffer = 100 \n",
    "# key for batching -? [index, current seq length]\n",
    "start = [0, len(x_train[0])] \n",
    "# batch\n",
    "for i, val in enumerate(x_train):\n",
    "    # if seq length is greater than batch_buffer create batch \n",
    "    if len(val) - start[1] > batch_buffer:\n",
    "        # create batch\n",
    "        x_batches.append(x_train[start[0] : i])\n",
    "        y_batches.append(y_train[start[0] : i])\n",
    "        # update index and current seq length\n",
    "        start[0] = i\n",
    "        start[1] = len(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66968"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding\n",
    "\n",
    "# padded batches \n",
    "px_batches = []\n",
    "unique = set()\n",
    "# apply padding per batch\n",
    "for batch in x_batches:\n",
    "    p_batch = [] # current patted batch\n",
    "    # get maximal seq length for current batch\n",
    "    max_size = max(len(seq) for seq in batch)\n",
    "    # loop over seq in batch\n",
    "    for seq in batch:\n",
    "        unique.update(seq)\n",
    "        # apply padding to seq and appedn\n",
    "        p_batch.append(seq + [0]*(max_size - len(seq)))\n",
    "    # append padded batch to padded batches\n",
    "    px_batches.append(p_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,1], [2,2], [3,3]])\n",
    "x[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elman(nn.Module):\n",
    "    \n",
    "    def __init__(self, insize=300, outsize=300, hsize=300):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(insize, hsize)\n",
    "        self.lin2 = nn.Linear(hsize, outsize) \n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # batch, len of sequence, embedding \n",
    "        b, t, e = x.size()\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(b, e, dtype=torch.float)\n",
    "        \n",
    "        prev_h = None\n",
    "        outs = []\n",
    "        # range over time \n",
    "        for i in range(t):\n",
    "            # inp = torch.cat([x[:, i, :], hidden], dim=1)\n",
    "            inp = []\n",
    "            # Compute first pass \n",
    "            xi = self.lin1(x[:, i , :])\n",
    "            \n",
    "            # manage hidden values \n",
    "            if prev_h is not None:\n",
    "                xh = xi \n",
    "            else:\n",
    "                xh = xi + prev_h\n",
    "\n",
    "            # update hidden states \n",
    "            xh = np.tanh(xh)\n",
    "            prev_h = xh\n",
    "\n",
    "            hidden = xh\n",
    "\n",
    "            # get outputs from sequence \n",
    "            out = self.lin2(xh)\n",
    "            outs.append(out[:, None, :])\n",
    "\n",
    "        return torch.cat(outs, dim=1), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elman(nn.Module):\n",
    "    \n",
    "    def __init__(self, insize=300, outsize=300, hsize=300):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin1 = nn.Linear(insize, hsize)\n",
    "        self.lin2 = nn.Linear(hsize, outsize) \n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # batch, len of sequence, embedding \n",
    "        b, t, e = x.size()\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(b, e, dtype=torch.float)\n",
    "        \n",
    "        outs = []\n",
    "        # range over time \n",
    "        for i in range(t):\n",
    "            inp = torch.cat([x[:, i, :], hidden], dim=1)\n",
    "\n",
    "            # Compute first pass \n",
    "            xi = self.lin1(inp)\n",
    "\n",
    "            # hidden \n",
    "            xh = np.tanh(xi)\n",
    "            hidden = xh\n",
    "\n",
    "            # get outputs from sequence \n",
    "            out = self.lin2(xh)\n",
    "\n",
    "            outs.append(out[:, None, :])\n",
    "\n",
    "        return torch.cat(outs, dim=1), hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman Network Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_batches_tens = [torch.tensor(i) for i in px_batches]\n",
    "y_batches_tens = [torch.tensor(i, dtype = torch.float32) for i in y_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2775, 110, 150])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(len(i2w), embedding_dim = 150)\n",
    "x_emb = emb(px_batches_tens[0])\n",
    "x_emb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2775, 110])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px_batches_tens[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  batch , lenght , embedding \n",
    "# embedding size = number of unique tokens in a batch = input size \n",
    "#  seq size = \n",
    "# embedding\n",
    "class ELMAN(nn.Module):\n",
    "    def __init__(self,embedding_size, hidden_size, output_size, dropout): \n",
    "                #  input_size, hidden_size, num_classes):\n",
    "        super(ELMAN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding(embedding_size, embedding_dim = 150)\n",
    "        self.rnn = nn.RNN(150, hidden_size, dropout = dropout, batch_first = True)\n",
    "        self.lin1 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h0):\n",
    "        # create emebeddings\n",
    "        x_emb = self.emb(x)\n",
    "\n",
    "        # pass through rnn\n",
    "        out, _ = self.rnn(x_emb, h0)\n",
    "\n",
    "        # predict\n",
    "        out = self.lin1(out[:, -1]) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs for network and hyperparameters\n",
    "embedding_size = len(i2w)\n",
    "hidden_size = 300\n",
    "output_size = 1\n",
    "alpha = 0.003\n",
    "epochs = 10 \n",
    "batch_size = len(px_batches_tens)\n",
    "\n",
    "def train_rnn(px_batches_tens, y_batches_tens, embedding_size, hidden_size, output_size, alpha, epochs, batch_size):\n",
    "    #inti network\n",
    "    rnn = ELMAN(embedding_size, hidden_size, output_size, dropout = 0)\n",
    "\n",
    "    # optimizers \n",
    "    obj_func = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), alpha)\n",
    "\n",
    "    e_loss = {\"loss\": [], \"norm_loss\": []}\n",
    "    for epoch in range(epochs):\n",
    "        batch_loss = 0.0\n",
    "        for idx, batch in enumerate(px_batches_tens):\n",
    "            h0 = torch.zeros(1, batch.shape[0], hidden_size) \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get network output \n",
    "            output = rnn(batch, h0)\n",
    "        \n",
    "            # get loss \n",
    "            loss = obj_func(output, y_batches_tens[idx])\n",
    "            \n",
    "            # update network\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update batch loss\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch}:\\nBatch loss: {batch_loss}, normalized loss: {batch_loss/batch_size}\")\n",
    "        # store loss\n",
    "        e_loss[\"loss\"].append(batch_loss)\n",
    "        e_loss[\"norm_loss\"].append(batch_loss/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rnn(px_batches_tens, y_batches_tens, embedding_size, hidden_size, output_size, alpha, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, dropout): \n",
    "                #  input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding(embedding_size, embedding_dim = 150)\n",
    "        self.rnn = nn.LSTM(150, hidden_size, dropout = dropout, batch_first=True)\n",
    "        self.lin1 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        # create emebeddings\n",
    "        x_emb = self.emb(x)\n",
    "\n",
    "        # pass through rnn\n",
    "        out, _ = self.rnn(x_emb, states)\n",
    "\n",
    "        # predict\n",
    "        out = self.lin1(out[:, -1]) \n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of batch tensor(0.4071, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# inputs for network and hyperparameters\n",
    "embedding_size = len(i2w)\n",
    "hidden_size = 300\n",
    "output_size = 1\n",
    "alpha = 0.003\n",
    "epochs = 10 \n",
    "num_layers = 1\n",
    "batch_size = len(px_batches_tens)\n",
    "\n",
    "#inti network\n",
    "lstm = LSTM(embedding_size, hidden_size, output_size, dropout = 0)\n",
    "\n",
    "# optimizers \n",
    "obj_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), alpha)\n",
    "\n",
    "\n",
    "e_loss = {\"loss\": [], \"norm_loss\": []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_loss = 0.0  \n",
    "    for idx, batch in enumerate(px_batches_tens):\n",
    "        # initialize hidden state and cell state \n",
    "        h0 = torch.zeros(1, batch.shape[0], hidden_size) \n",
    "        c0 = torch.zeros(1, batch.shape[0], hidden_size)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get network output \n",
    "        output = lstm(batch, (h0,c0))\n",
    "\n",
    "        # get loss \n",
    "        loss = obj_func(output, y_batches_tens[idx])\n",
    "        print(\"Loss of batch\",loss)\n",
    "        # update network\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update batch loss\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "    e_loss.append(batch_loss)\n",
    "    print(f\"Epoch {epoch}:\\nBatch loss: {batch_loss}, normalized loss: {batch_loss/batch_size}\")\n",
    "    # store loss\n",
    "    e_loss[\"loss\"].append(batch_loss)\n",
    "    e_loss[\"norm_loss\"].append(batch_loss/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, px_batches_tens, y_batches_tens, alpha, epochs, opt):\n",
    "\n",
    "    embedding_size = len(i2w)\n",
    "    hidden_size = 300\n",
    "    output_size = 1\n",
    "    alpha = 0.003\n",
    "    epochs = 10 \n",
    "    num_layers = 1\n",
    "    batch_size = len(px_batches_tens)\n",
    "\n",
    "    # inti network\n",
    "    rnn = model(embedding_size, hidden_size, output_size, dropout = 0)\n",
    "\n",
    "    # set objective function \n",
    "    obj_func = nn.MSELoss()\n",
    "\n",
    "    # set optimizer \n",
    "    if opt == 0:\n",
    "        optimizer = torch.optim.Adam(rnn.parameters(), alpha)\n",
    "    elif opt == 1:\n",
    "        optimizer = torch.optim.Adadelta(rnn.parameters(), alpha)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(rnn.parameters(), alpha)\n",
    "   \n",
    "    e_loss = {\"loss\": [], \"norm_loss\": []}\n",
    "    for epoch in range(epochs):\n",
    "        batch_loss = 0.0\n",
    "        for idx, batch in enumerate(px_batches_tens):\n",
    "            h0 = torch.zeros(num_layers, batch.shape[0], hidden_size) \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get network output \n",
    "            output = rnn(batch, h0)\n",
    "        \n",
    "            # get loss \n",
    "            loss = obj_func(output, y_batches_tens[idx])\n",
    "            \n",
    "            # update network\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update batch loss\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch}:\\nBatch loss: {batch_loss}, normalized loss: {batch_loss/batch_size}\")\n",
    "        # store loss\n",
    "        e_loss[\"loss\"].append(batch_loss)\n",
    "        e_loss[\"norm_loss\"].append(batch_loss/batch_size)\n",
    "    \n",
    "    return e_loss[\"norm_loss\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayes_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#module for optimization\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction\n",
    "# module for logging data \n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "# module for retriving datat \n",
    "from bayes_opt.util import load_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP Hyperparameter tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Q1Q2Q3 import Seq2SeqModel\n",
    "# parameter bounds\n",
    "pbounds = {\"alpha\" : ( 0.01, 0.003), \"epochs\": (100, 125), \"opt\": (-0.5, 2.5)}\n",
    "\n",
    "# define wrapped funciton\n",
    "def train_wrapper(alpha, epochs, opt):\n",
    "    opt = int(round(opt))\n",
    "    return train_rnn(Seq2SeqModel, px_batches_tens, y_batches_tens, alpha, epochs, opt = opt)\n",
    "\n",
    "# create instance of optimizer \n",
    "optimizer_bayes = BayesianOptimization(\n",
    "    f = train_wrapper,\n",
    "    pbounds = pbounds,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "# create UtilityFunction object for aqu. function\n",
    "utility = UtilityFunction(kind = \"ei\", xi= 0.02)\n",
    "\n",
    "# set gaussian process parameter\n",
    "optimizer_bayes.set_gp_params(alpha = 1e-6)\n",
    "\n",
    "# create logger \n",
    "logger = JSONLogger(path = \"./tunning1.log\")\n",
    "optimizer_bayes.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# initial search \n",
    "optimizer_bayes.maximize(\n",
    "    init_points = 5, # number of random explorations before bayes_opt\n",
    "    n_iter = 15, # number of bayes_opt iterations\n",
    ")\n",
    "\n",
    "# print out the data from the initial run to check if bounds need update \n",
    "for i, param in enumerate(optimizer_bayes.res):\n",
    "    print(f\"Iteration {i}: \\n\\t {param}\")\n",
    "\n",
    "# get best parameter\n",
    "print(\"Best Parameters found: \")\n",
    "print(optimizer_bayes.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELMAN hyperparameter tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter bounds\n",
    "pbounds = {\"alpha\" : ( 0.01, 0.003), \"epochs\": (100, 125), \"opt\": (-0.5, 2.5)}\n",
    "\n",
    "# define wrapped funciton\n",
    "def train_wrapper(alpha, epochs, opt):\n",
    "    opt = int(round(opt))\n",
    "    return train_rnn(ELMAN, px_batches_tens, y_batches_tens, alpha, epochs, opt = opt)\n",
    "\n",
    "# create instance of optimizer \n",
    "optimizer_bayes = BayesianOptimization(\n",
    "    f = train_wrapper,\n",
    "    pbounds = pbounds,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "# create UtilityFunction object for aqu. function\n",
    "utility = UtilityFunction(kind = \"ei\", xi= 0.02)\n",
    "\n",
    "# set gaussian process parameter\n",
    "optimizer_bayes.set_gp_params(alpha = 1e-6)\n",
    "\n",
    "# create logger \n",
    "logger = JSONLogger(path = \"./tunning1.log\")\n",
    "optimizer_bayes.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# initial search \n",
    "optimizer_bayes.maximize(\n",
    "    init_points = 5, # number of random explorations before bayes_opt\n",
    "    n_iter = 15, # number of bayes_opt iterations\n",
    ")\n",
    "\n",
    "# print out the data from the initial run to check if bounds need update \n",
    "for i, param in enumerate(optimizer_bayes.res):\n",
    "    print(f\"Iteration {i}: \\n\\t {param}\")\n",
    "\n",
    "# get best parameter\n",
    "print(\"Best Parameters found: \")\n",
    "print(optimizer_bayes.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM hyperparameter tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([2775])) that is different to the input size (torch.Size([2775, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m optimizer_bayes\u001b[38;5;241m.\u001b[39msubscribe(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP, logger)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# initial search \u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43moptimizer_bayes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_points\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# number of random explorations before bayes_opt\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# number of bayes_opt iterations\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# print out the data from the initial run to check if bounds need update \u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(optimizer_bayes\u001b[38;5;241m.\u001b[39mres):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/bayes_opt/bayesian_optimization.py:311\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    309\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest(util)\n\u001b[1;32m    310\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/bayes_opt/bayesian_optimization.py:208\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39madd(params)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/bayes_opt/target_space.py:236\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    234\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_as_array(params)\n\u001b[1;32m    235\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keys, x))\n\u001b[0;32m--> 236\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister(x, target)\n",
      "Cell \u001b[0;32mIn[128], line 7\u001b[0m, in \u001b[0;36mtrain_wrapper\u001b[0;34m(alpha, epochs, adam)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_wrapper\u001b[39m(alpha, epochs, adam):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mELMAN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpx_batches_tens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batches_tens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madam\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madam\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[127], line 29\u001b[0m, in \u001b[0;36mtrain_rnn\u001b[0;34m(model, px_batches_tens, y_batches_tens, alpha, epochs, adam)\u001b[0m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# get network output \u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# get loss \u001b[39;00m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m obj_func(output, y_batches_tens[idx])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[96], line 19\u001b[0m, in \u001b[0;36mELMAN.forward\u001b[0;34m(self, x, h0)\u001b[0m\n\u001b[1;32m     16\u001b[0m x_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb(x)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# pass through rnn\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[1;32m     22\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin1(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/rnn.py:714\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 714\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_tanh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\n\u001b[1;32m    727\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    728\u001b[0m             hx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[1;32m    736\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# parameter bounds\n",
    "pbounds = {\"alpha\" : ( 0.01, 0.003), \"epochs\": (100, 125), \"opt\": (-0.5, 2.5)}\n",
    "\n",
    "# define wrapped funciton\n",
    "def train_wrapper(alpha, epochs, opt):\n",
    "    opt = int(round(opt))\n",
    "    return train_rnn(LSTM, px_batches_tens, y_batches_tens, alpha, epochs, opt = opt)\n",
    "\n",
    "# create instance of optimizer \n",
    "optimizer_bayes = BayesianOptimization(\n",
    "    f = train_wrapper,\n",
    "    pbounds = pbounds,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "# create UtilityFunction object for aqu. function\n",
    "utility = UtilityFunction(kind = \"ei\", xi= 0.02)\n",
    "\n",
    "# set gaussian process parameter\n",
    "optimizer_bayes.set_gp_params(alpha = 1e-6)\n",
    "\n",
    "# create logger \n",
    "logger = JSONLogger(path = \"./tunning1.log\")\n",
    "optimizer_bayes.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# initial search \n",
    "optimizer_bayes.maximize(\n",
    "    init_points = 5, # number of random explorations before bayes_opt\n",
    "    n_iter = 15, # number of bayes_opt iterations\n",
    ")\n",
    "\n",
    "# print out the data from the initial run to check if bounds need update \n",
    "for i, param in enumerate(optimizer_bayes.res):\n",
    "    print(f\"Iteration {i}: \\n\\t {param}\")\n",
    "\n",
    "# get best parameter\n",
    "print(\"Best Parameters found: \")\n",
    "print(optimizer_bayes.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m opt_bayes\u001b[38;5;241m.\u001b[39msubscribe(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP, logger)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# initial search \u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mopt_bayes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_points\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# number of random explorations before bayes_opt\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# number of bayes_opt iterations\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print out the data from the initial run to check if bounds need update \u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(optimizer1\u001b[38;5;241m.\u001b[39mres):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/bayes_opt/bayesian_optimization.py:284\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prime_subscriptions()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_START)\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prime_queue\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m old_params_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m([param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m [acq, kappa, kappa_decay, kappa_decay_delay, xi]])\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m old_params_used \u001b[38;5;129;01mor\u001b[39;00m gp_params:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/bayes_opt/bayesian_optimization.py:241\u001b[0m, in \u001b[0;36mBayesianOptimization._prime_queue\u001b[0;34m(self, init_points)\u001b[0m\n\u001b[1;32m    238\u001b[0m     init_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(init_points, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(init_points):\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/bayes_opt/target_space.py:264\u001b[0m, in \u001b[0;36mTargetSpace.random_sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreates random points within the bounds of the space.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03marray([[ 55.33253689,   0.54488318]])\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim))\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, (lower, upper) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds):\n\u001b[1;32m    265\u001b[0m     data\u001b[38;5;241m.\u001b[39mT[col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\u001b[38;5;241m.\u001b[39muniform(lower, upper, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "optimizer1 = torch.optim.Adadelta(elman.parameters(), alpha)\n",
    "optimizer2 = torch.optim.Adam(elman.parameters(), alpha)\n",
    "\n",
    "pbounds = {\"alpha\" : (0.1, 0.01, 0.003), \"epochs\": (75, 100, 125)}\n",
    "\n",
    "# define wrapped funciton\n",
    "def train_wrapper(alpha, epochs):\n",
    "    return train_rnn(ELMAN, optimizer1, px_batches_tens, y_batches_tens, alpha, epochs)\n",
    "\n",
    "# create instance of optimizer \n",
    "opt_bayes = BayesianOptimization(\n",
    "    f = train_wrapper,\n",
    "    pbounds = pbounds,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "# create UtilityFunction object for aqu. function\n",
    "utility = UtilityFunction(kind = \"ei\", xi= 0.02)\n",
    "\n",
    "# set gaussian process parameter\n",
    "opt_bayes.set_gp_params(alpha = 1e-6)\n",
    "\n",
    "# create logger \n",
    "logger = JSONLogger(path = \"./tunning1.log\")\n",
    "opt_bayes.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# initial search \n",
    "opt_bayes.maximize(\n",
    "    init_points = 5, # number of random explorations before bayes_opt\n",
    "    n_iter = 15, # number of bayes_opt iterations\n",
    ")\n",
    "\n",
    "# print out the data from the initial run to check if bounds need update \n",
    "for i, param in enumerate(opt_bayes.res):\n",
    "    print(f\"Iteration {i}: \\n\\t {param}\")\n",
    "\n",
    "# get best parameter\n",
    "print(\"Best Parameters found: \")\n",
    "print(opt_bayes.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([169])) that is different to the input size (torch.Size([169, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([139])) that is different to the input size (torch.Size([139, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([53])) that is different to the input size (torch.Size([53, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/akshayeiyer/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Batch loss: 5.483054695650935, normalized loss: 0.34269091847818345\n",
      "Epoch 1:\n",
      "Batch loss: 5.483054695650935, normalized loss: 0.34269091847818345\n",
      "Epoch 2:\n",
      "Batch loss: 5.483054695650935, normalized loss: 0.34269091847818345\n",
      "Epoch 3:\n",
      "Batch loss: 5.483054695650935, normalized loss: 0.34269091847818345\n",
      "Epoch 4:\n",
      "Batch loss: 5.483054695650935, normalized loss: 0.34269091847818345\n",
      "Epoch 5:\n",
      "Batch loss: 5.483054695650935, normalized loss: 0.34269091847818345\n",
      "Epoch 6:\n",
      "Batch loss: 5.483054695650935, normalized loss: 0.34269091847818345\n",
      "Epoch 7:\n",
      "Batch loss: 5.483054695650935, normalized loss: 0.34269091847818345\n",
      "Epoch 8:\n",
      "Batch loss: 5.483054695650935, normalized loss: 0.34269091847818345\n",
      "Epoch 9:\n",
      "Batch loss: 5.483054695650935, normalized loss: 0.34269091847818345\n",
      "Epoch 0:\n",
      "Batch loss: 13.921797037124634, normalized loss: 0.8701123148202896\n",
      "Epoch 1:\n",
      "Batch loss: 13.921797037124634, normalized loss: 0.8701123148202896\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_rnn(ELMAN, optimizer, px_batches_tens, y_batches_tens, alpha, epochs)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha, optimizer, epochs \u001b[38;5;129;01min\u001b[39;00m parameter_tuning:\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mtrain_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[112], line 7\u001b[0m, in \u001b[0;36mtrain_wrapper\u001b[0;34m(alpha, optimizer, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_wrapper\u001b[39m(alpha, optimizer, epochs):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mELMAN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpx_batches_tens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batches_tens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[100], line 32\u001b[0m, in \u001b[0;36mtrain_rnn\u001b[0;34m(model, optimizer, px_batches_tens, y_batches_tens, alpha, epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m obj_func(output, y_batches_tens[idx])\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# update network\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# update batch loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer1 = torch.optim.Adadelta(elman.parameters(), alpha)\n",
    "optimizer2 = torch.optim.Adam(elman.parameters(), alpha)\n",
    "\n",
    "# pbounds = {\"alpha\" : (0.1, 0.01, 0.003), \"optimizer\" : (optimizer1, optimizer2), \"epochs\": (75, 100, 125)}\n",
    "parameter_tuning = [(0.001,optimizer1, 75),(0.01, optimizer2, 100),(0.001, optimizer1, 125)]\n",
    "def train_wrapper(alpha, optimizer, epochs):\n",
    "    return train_rnn(ELMAN, optimizer, px_batches_tens, y_batches_tens, alpha, epochs)\n",
    "\n",
    "for alpha, optimizer, epochs in parameter_tuning:\n",
    "    train_wrapper(alpha, optimizer, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, optimizer, px_batches_tens, y_batches_tens, alpha, epochs):\n",
    "\n",
    "    embedding_size = len(i2w)\n",
    "    hidden_size = 300\n",
    "    output_size = 1\n",
    "    alpha = 0.003\n",
    "    epochs = 10 \n",
    "    num_layers = 1\n",
    "    batch_size = len(px_batches_tens)\n",
    "\n",
    "    #inti network\n",
    "    rnn = model(embedding_size, hidden_size, output_size, dropout = 0)\n",
    "\n",
    "    # optimizers \n",
    "    obj_func = nn.MSELoss()\n",
    "    optimizer = optimizer\n",
    "\n",
    "    e_loss = {\"loss\": [], \"norm_loss\": []}\n",
    "    for epoch in range(epochs):\n",
    "        batch_loss = 0.0\n",
    "        for idx, batch in enumerate(px_batches_tens):\n",
    "            h0 = torch.zeros(num_layers, batch.shape[0], hidden_size) \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get network output \n",
    "            output = rnn(batch, h0)\n",
    "        \n",
    "            # get loss \n",
    "            loss = obj_func(output, y_batches_tens[idx])\n",
    "            \n",
    "            # update network\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update batch loss\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch}:\\nBatch loss: {batch_loss}, normalized loss: {batch_loss/batch_size}\")\n",
    "        # store loss\n",
    "        e_loss[\"loss\"].append(batch_loss)\n",
    "        e_loss[\"norm_loss\"].append(batch_loss/batch_size)\n",
    "    \n",
    "    return e_loss[\"norm_loss\"][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#module for optimization\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction\n",
    "# module for logging data \n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "# module for retriving datat \n",
    "from bayes_opt.util import load_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.Adadelta(lstm.parameters(), alpha)\n",
    "optimizer2 = torch.optim.bayesopt(lstm.parameters(), alpha)\n",
    "\n",
    "pbounds = {\"alpha\" : (0.1, 0.01, 0.003), \"optimizer\" : (optimizer1, optimizer2), \"epochs\": (75, 100, 125)}\n",
    "\n",
    "# define wrapped funciton\n",
    "def train_wrapper(alpha, optimizer, epochs):\n",
    "    return train_rnn(ELMAN, optimizer, px_batches_tens, y_batches_tens, alpha, epochs)\n",
    "\n",
    "# create instance of optimizer \n",
    "optimizer1 = BayesianOptimization(\n",
    "    f = train_wrapper,\n",
    "    pbounds = pbounds,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "# create UtilityFunction object for aqu. function\n",
    "utility = UtilityFunction(kind = \"ei\", xi= 0.02)\n",
    "\n",
    "# set gaussian process parameter\n",
    "optimizer1.set_gp_params(alpha = 1e-6)\n",
    "\n",
    "# create logger \n",
    "logger = JSONLogger(path = \"./tunning1.log\")\n",
    "optimizer1.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# initial search \n",
    "optimizer1.maximize(\n",
    "    init_points = 5, # number of random explorations before bayes_opt\n",
    "    n_iter = 15, # number of bayes_opt iterations\n",
    ")\n",
    "\n",
    "# print out the data from the initial run to check if bounds need update \n",
    "for i, param in enumerate(optimizer1.res):\n",
    "    print(f\"Iteration {i}: \\n\\t {param}\")\n",
    "\n",
    "# get best parameter\n",
    "print(\"Best Parameters found: \")\n",
    "print(optimizer1.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.Adadelta(lstm.parameters(), alpha)\n",
    "optimizer2 = torch.optim.bayesopt(lstm.parameters(), alpha)\n",
    "\n",
    "pbounds = {\"alpha\" : (0.1, 0.01, 0.003), \"optimizer\" : (optimizer1, optimizer2), \"epochs\": (75, 100, 125)}\n",
    "\n",
    "# define wrapped funciton\n",
    "def train_wrapper(alpha, optimizer, epochs):\n",
    "    return train_rnn(LSTM, optimizer, px_batches_tens, y_batches_tens, alpha, epochs)\n",
    "\n",
    "# create instance of optimizer \n",
    "optimizer1 = BayesianOptimization(\n",
    "    f = train_wrapper,\n",
    "    pbounds = pbounds,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "# create UtilityFunction object for aqu. function\n",
    "utility = UtilityFunction(kind = \"ei\", xi= 0.02)\n",
    "\n",
    "# set gaussian process parameter\n",
    "optimizer1.set_gp_params(alpha = 1e-6)\n",
    "\n",
    "# create logger \n",
    "logger = JSONLogger(path = \"./tunning1.log\")\n",
    "optimizer1.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "# initial search \n",
    "optimizer1.maximize(\n",
    "    init_points = 5, # number of random explorations before bayes_opt\n",
    "    n_iter = 15, # number of bayes_opt iterations\n",
    ")\n",
    "\n",
    "# print out the data from the initial run to check if bounds need update \n",
    "for i, param in enumerate(optimizer1.res):\n",
    "    print(f\"Iteration {i}: \\n\\t {param}\")\n",
    "\n",
    "# get best parameter\n",
    "print(\"Best Parameters found: \")\n",
    "print(optimizer1.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
