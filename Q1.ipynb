{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-07T14:11:33.083515Z",
     "start_time": "2024-12-07T14:11:32.250591Z"
    }
   },
   "source": [
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "import wget\n",
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10cdcf770>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T14:11:36.747288Z",
     "start_time": "2024-12-07T14:11:36.742067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IMDB_URL = 'http://dlvu.github.io/data/imdb.{}.pkl.gz'\n",
    "IMDB_FILE = 'imdb.{}.pkl.gz'\n",
    "\n",
    "PAD, START, END, UNK = '.pad', '.start', '.end', '.unk'\n",
    "\n",
    "def load_imdb(final=False, val=5000, seed=0, voc=None, char=False):\n",
    "\n",
    "    cst = 'char' if char else 'word'\n",
    "\n",
    "    imdb_url = IMDB_URL.format(cst)\n",
    "    imdb_file = IMDB_FILE.format(cst)\n",
    "\n",
    "    if not os.path.exists(imdb_file):\n",
    "        wget.download(imdb_url)\n",
    "\n",
    "    with gzip.open(imdb_file) as file:\n",
    "        sequences, labels, i2w, w2i = pickle.load(file)\n",
    "\n",
    "    if voc is not None and voc < len(i2w):\n",
    "        nw_sequences = {}\n",
    "\n",
    "        i2w = i2w[:voc]\n",
    "        w2i = {w: i for i, w in enumerate(i2w)}\n",
    "\n",
    "        mx, unk = voc, w2i['.unk']\n",
    "        for key, seqs in sequences.items():\n",
    "            nw_sequences[key] = []\n",
    "            for seq in seqs:\n",
    "                seq = [s if s < mx else unk for s in seq]\n",
    "                nw_sequences[key].append(seq)\n",
    "\n",
    "        sequences = nw_sequences\n",
    "\n",
    "    if final:\n",
    "        return (sequences['train'], labels['train']), (sequences['test'], labels['test']), (i2w, w2i), 2\n",
    "\n",
    "    # Make a validation split\n",
    "    random.seed(seed)\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    x_val, y_val = [], []\n",
    "\n",
    "    val_ind = set( random.sample(range(len(sequences['train'])), k=val) )\n",
    "    for i, (s, l) in enumerate(zip(sequences['train'], labels['train'])):\n",
    "        if i in val_ind:\n",
    "            x_val.append(s)\n",
    "            y_val.append(l)\n",
    "        else:\n",
    "            x_train.append(s)\n",
    "            y_train.append(l)\n",
    "\n",
    "    return (x_train, y_train), \\\n",
    "           (x_val, y_val), \\\n",
    "           (i2w, w2i), 2"
   ],
   "id": "51a80f2fa0208cfa",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T14:11:39.526567Z",
     "start_time": "2024-12-07T14:11:39.210006Z"
    }
   },
   "cell_type": "code",
   "source": "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = load_imdb(final=False)",
   "id": "647d0a89ce44a1e4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T14:11:40.463855Z",
     "start_time": "2024-12-07T14:11:40.458929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_and_convert(sequences: List[List[int]], w2i: Dict[str, int],\n",
    "                   max_length: int = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pads a list of sequences to a fixed length and converts them to a PyTorch tensor.\n",
    "\n",
    "    Args:\n",
    "        sequences (List[List[int]]): A batch of sequences, where each sequence is a list of integer indices.\n",
    "        w2i (Dict[str, int]): A dictionary mapping words to their integer indices.\n",
    "        max_length (int, optional): The length to pad the sequences to. If None, uses the length of the longest sequence in the batch.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (batch_size, max_length) containing the padded sequences.\n",
    "    \"\"\"\n",
    "    # Retrieve the padding index from the w2i dictionary\n",
    "    pad_idx = w2i.get('.pad')\n",
    "    if pad_idx is None:\n",
    "        raise ValueError(\"The padding token '.pad' is not found in the w2i dictionary.\")\n",
    "\n",
    "    # Determine the maximum length for padding\n",
    "    if max_length is None:\n",
    "        max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "    # Initialize a list to hold the padded sequences\n",
    "    padded_sequences = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        # Calculate the number of padding tokens needed\n",
    "        padding_needed = max_length - len(seq)\n",
    "\n",
    "        if padding_needed < 0:\n",
    "            raise ValueError(\"A sequence is longer than the specified max_length.\")\n",
    "\n",
    "        # Pad the sequence with pad_idx\n",
    "        padded_seq = seq + [pad_idx] * padding_needed\n",
    "        padded_sequences.append(padded_seq)\n",
    "\n",
    "    # Convert the list of padded sequences to a PyTorch tensor with dtype torch.long\n",
    "    batch_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "    return batch_tensor\n",
    "\n",
    "def create_batches(sequences: List[List[int]], labels: List[int],\n",
    "                  batch_size: int, w2i: Dict[str, int]) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Splits the data into batches, pads each batch, and converts them to tensors.\n",
    "\n",
    "    Args:\n",
    "        sequences (List[List[int]]): List of all sequences.\n",
    "        labels (List[int]): Corresponding labels for each sequence.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        w2i (Dict[str, int]): Dictionary mapping words to their integer indices.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[torch.Tensor, torch.Tensor]]: A list of tuples, each containing padded sequences and their labels as tensors.\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    total_samples = len(sequences)\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size  # Ceiling division\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, total_samples)\n",
    "        batch_sequences = sequences[start_idx:end_idx]\n",
    "        batch_labels = labels[start_idx:end_idx]\n",
    "\n",
    "        # Pad and convert sequences\n",
    "        padded_sequences = pad_and_convert(batch_sequences, w2i)\n",
    "\n",
    "        # Convert labels to tensor\n",
    "        labels_tensor = torch.tensor(batch_labels, dtype=torch.long)\n",
    "\n",
    "        batches.append((padded_sequences, labels_tensor))\n",
    "\n",
    "    return batches"
   ],
   "id": "1a7f716edc8f708e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T14:12:08.615784Z",
     "start_time": "2024-12-07T14:12:08.423528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "train_batches = create_batches(x_train, y_train, batch_size, w2i)\n",
    "val_batches = create_batches(x_val, y_val, batch_size, w2i)"
   ],
   "id": "cc229015f548b199",
   "outputs": [],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
