{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adadelta(lstm.parameters(), alpha)\n",
    "optimizer = torch.optim.bayesopt(lstm.parameters(), alpha)\n",
    "\n",
    "def train_rnn(px_batches_tens, y_batches_tens, embedding_size, hidden_size, output_size, alpha, epochs, batch_size):\n",
    "    #inti network\n",
    "    rnn = ELMAN(embedding_size, hidden_size, output_size, dropout = 0)\n",
    "\n",
    "    # optimizers \n",
    "    obj_func = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), alpha)\n",
    "\n",
    "    e_loss = {\"loss\": [], \"norm_loss\": []}\n",
    "    for epoch in range(epochs):\n",
    "        batch_loss = 0.0\n",
    "        for idx, batch in enumerate(px_batches_tens):\n",
    "            h0 = torch.zeros(1, batch.shape[0], hidden_size) \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get network output \n",
    "            output = rnn(batch, h0)\n",
    "        \n",
    "            # get loss \n",
    "            loss = obj_func(output, y_batches_tens[idx])\n",
    "            \n",
    "            # update network\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update batch loss\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch}:\\nBatch loss: {batch_loss}, normalized loss: {batch_loss/batch_size}\")\n",
    "        # store loss\n",
    "        e_loss[\"loss\"].append(batch_loss)\n",
    "        e_loss[\"norm_loss\"].append(batch_loss/batch_size)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
