{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget, os, gzip, pickle, random, re, sys\n",
    "\n",
    "IMDB_URL = 'http://dlvu.github.io/data/imdb.{}.pkl.gz'\n",
    "IMDB_FILE = 'imdb.{}.pkl.gz'\n",
    "\n",
    "PAD, START, END, UNK = '.pad', '.start', '.end', '.unk'\n",
    "\n",
    "def load_imdb(final=False, val=5000, seed=0, voc=None, char=False):\n",
    "\n",
    "    cst = 'char' if char else 'word'\n",
    "\n",
    "    imdb_url = IMDB_URL.format(cst)\n",
    "    imdb_file = IMDB_FILE.format(cst)\n",
    "\n",
    "    if not os.path.exists(imdb_file):\n",
    "        wget.download(imdb_url)\n",
    "\n",
    "    with gzip.open(imdb_file) as file:\n",
    "        sequences, labels, i2w, w2i = pickle.load(file)\n",
    "\n",
    "    if voc is not None and voc < len(i2w):\n",
    "        nw_sequences = {}\n",
    "\n",
    "        i2w = i2w[:voc]\n",
    "        w2i = {w: i for i, w in enumerate(i2w)}\n",
    "\n",
    "        mx, unk = voc, w2i['.unk']\n",
    "        for key, seqs in sequences.items():\n",
    "            nw_sequences[key] = []\n",
    "            for seq in seqs:\n",
    "                seq = [s if s < mx else unk for s in seq]\n",
    "                nw_sequences[key].append(seq)\n",
    "\n",
    "        sequences = nw_sequences\n",
    "\n",
    "    if final:\n",
    "        return (sequences['train'], labels['train']), (sequences['test'], labels['test']), (i2w, w2i), 2\n",
    "\n",
    "    # Make a validation split\n",
    "    random.seed(seed)\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    x_val, y_val = [], []\n",
    "\n",
    "    val_ind = set( random.sample(range(len(sequences['train'])), k=val) )\n",
    "    for i, (s, l) in enumerate(zip(sequences['train'], labels['train'])):\n",
    "        if i in val_ind:\n",
    "            x_val.append(s)\n",
    "            y_val.append(l)\n",
    "        else:\n",
    "            x_train.append(s)\n",
    "            y_train.append(l)\n",
    "\n",
    "    return (x_train, y_train), \\\n",
    "           (x_val, y_val), \\\n",
    "           (i2w, w2i), 2\n",
    "\n",
    "\n",
    "def gen_sentence(sent, g):\n",
    "\n",
    "    symb = '_[a-z]*'\n",
    "\n",
    "    while True:\n",
    "\n",
    "        match = re.search(symb, sent)\n",
    "        if match is None:\n",
    "            return sent\n",
    "\n",
    "        s = match.span()\n",
    "        sent = sent[:s[0]] + random.choice(g[sent[s[0]:s[1]]]) + sent[s[1]:]\n",
    "\n",
    "def gen_dyck(p):\n",
    "    open = 1\n",
    "    sent = '('\n",
    "    while open > 0:\n",
    "        if random.random() < p:\n",
    "            sent += '('\n",
    "            open += 1\n",
    "        else:\n",
    "            sent += ')'\n",
    "            open -= 1\n",
    "\n",
    "    return sent\n",
    "\n",
    "def gen_ndfa(p):\n",
    "\n",
    "    word = random.choice(['abc!', 'uvw!', 'klm!'])\n",
    "\n",
    "    s = ''\n",
    "    while True:\n",
    "        if random.random() < p:\n",
    "            return 's' + s + 's'\n",
    "        else:\n",
    "            s+= word\n",
    "\n",
    "def load_brackets(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='dyck')\n",
    "\n",
    "def load_ndfa(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='ndfa')\n",
    "\n",
    "def load_toy(n=50_000, char=True, seed=0, name='lang'):\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    if name == 'lang':\n",
    "        sent = '_s'\n",
    "\n",
    "        toy = {\n",
    "            '_s': ['_s _adv', '_np _vp', '_np _vp _prep _np', '_np _vp ( _prep _np )', '_np _vp _con _s' , '_np _vp ( _con _s )'],\n",
    "            '_adv': ['briefly', 'quickly', 'impatiently'],\n",
    "            '_np': ['a _noun', 'the _noun', 'a _adj _noun', 'the _adj _noun'],\n",
    "            '_prep': ['on', 'with', 'to'],\n",
    "            '_con' : ['while', 'but'],\n",
    "            '_noun': ['mouse', 'bunny', 'cat', 'dog', 'man', 'woman', 'person'],\n",
    "            '_vp': ['walked', 'walks', 'ran', 'runs', 'goes', 'went'],\n",
    "            '_adj': ['short', 'quick', 'busy', 'nice', 'gorgeous']\n",
    "        }\n",
    "\n",
    "        sentences = [ gen_sentence(sent, toy) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s : len(s))\n",
    "\n",
    "    elif name == 'dyck':\n",
    "\n",
    "        sentences = [gen_dyck(7./16.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    elif name == 'ndfa':\n",
    "\n",
    "        sentences = [gen_ndfa(1./4.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    else:\n",
    "        raise Exception(name)\n",
    "\n",
    "    tokens = set()\n",
    "    for s in sentences:\n",
    "\n",
    "        if char:\n",
    "            for c in s:\n",
    "                tokens.add(c)\n",
    "        else:\n",
    "            for w in s.split():\n",
    "                tokens.add(w)\n",
    "\n",
    "    i2t = [PAD, START, END, UNK] + list(tokens)\n",
    "    t2i = {t:i for i, t in enumerate(i2t)}\n",
    "\n",
    "    sequences = []\n",
    "    for s in sentences:\n",
    "        if char:\n",
    "            tok = list(s)\n",
    "        else:\n",
    "            tok = s.split()\n",
    "        sequences.append([t2i[t] for t in tok])\n",
    "\n",
    "    return sequences, (i2t, t2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load  NDFA  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, (i2w, w2i) = load_ndfa(n=150_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching and Padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(x_train):\n",
    "    # batching \n",
    "    x_batches = []\n",
    "    # cut of value for batches -> batches are created with sequences that contain a max diff of 100\n",
    "    batch_buffer = 15 \n",
    "    # key for batching -? [index, current seq length]\n",
    "    start = [0, len(x_train[0])] \n",
    "    # batch\n",
    "    for i, val in enumerate(x_train):\n",
    "        # if seq length is greater than batch_buffer create batch \n",
    "        if len(val) - start[1] > batch_buffer:\n",
    "            # create batch\n",
    "            x_batches.append(x_train[start[0] : i])\n",
    "            # update index and current seq length\n",
    "            start[0] = i\n",
    "            start[1] = len(val)\n",
    "    \n",
    "    return x_batches\n",
    "\n",
    "def padding(x_batches):\n",
    "    # padded batches \n",
    "    px_batches = []\n",
    "    py_batches = []\n",
    "    # apply padding per batch\n",
    "    for batch in x_batches:\n",
    "        xp_batch = [] # current patted batch\n",
    "        yp_batch = []\n",
    "\n",
    "        # get maximal seq length for current batch\n",
    "        max_size = max(len(seq) for seq in batch)\n",
    "        # loop over seq in batch\n",
    "        for seq in batch:\n",
    "            # apply padding, start and ending char to seq and apped\n",
    "            xp_batch.append([1] + seq + [0]*(max_size - len(seq)) + [2])\n",
    "            yp_batch.append([0] +  seq[1:] + [0]*(max_size - len(seq)) + [2, 0])\n",
    "\n",
    "            # yp_batch.append([0] + seq + [0]*(max_size - len(seq) ) + [2])\n",
    "            \n",
    "        # append padded batch to padded batches\n",
    "        px_batches.append(xp_batch)\n",
    "        py_batches.append(yp_batch)\n",
    "    \n",
    "    return px_batches, py_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get padded batched train\n",
    "x_padbatches, y_padbatches = padding(batch(x_train)) \n",
    "xtens_pb = [torch.tensor(batch) for batch in x_padbatches]\n",
    "ytens_pb = [torch.tensor(batch) for batch in y_padbatches]\n",
    "data_set = [(x, y) for x,y in zip(xtens_pb, ytens_pb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, embedding_dim, num_layers = 1): \n",
    "        #  input_size, hidden_size, num_layers, num_classes):\n",
    "        super(autoLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding(embedding_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers= num_layers, batch_first=True)\n",
    "        self.lin1 = nn.Linear(hidden_size, embedding_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        # create emebeddings\n",
    "        x_emb = self.emb(x)\n",
    "\n",
    "        # pass through rnn\n",
    "        out, _ = self.rnn(x_emb, states)\n",
    "\n",
    "        # predict\n",
    "        out = self.lin1(out) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoreg(xtens_pb, ytens_pb, embedding_size, hidden_size, epochs, alpha, num_layers, opt = \"sgd\"):\n",
    "    batch_size = len(xtens_pb)\n",
    "    embedding_dim = 150\n",
    "\n",
    "    # inti network\n",
    "    rnn = autoLSTM(embedding_size, hidden_size, embedding_dim, num_layers = num_layers)\n",
    "\n",
    "    # select optimizer\n",
    "    if opt == \"adam\":\n",
    "        optimizer = torch.optim.Adam(rnn.parameters(), alpha)\n",
    "    elif opt == \"adamdelta\":\n",
    "        optimizer = torch.optim.Adadelta(rnn.parameters(), alpha)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(rnn.parameters(), alpha)\n",
    "\n",
    "    # set objective function \n",
    "    obj_func = nn.CrossEntropyLoss() \n",
    "\n",
    "    data_set = [(x, y) for x,y in zip(xtens_pb, ytens_pb)]\n",
    "\n",
    "    # epoch loss \n",
    "    e_loss = {\"loss\": [], \"norm_loss\": []}\n",
    "    for epoch in range(epochs):\n",
    "        # shuffle training data \n",
    "        np.random.shuffle(data_set) \n",
    "\n",
    "        # init batch loss\n",
    "        batch_loss = 0.0\n",
    "        # loop over batches\n",
    "        for idx, batch in enumerate(data_set):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # settings for LSTM\n",
    "            h0 = torch.zeros(num_layers, batch[0].shape[0], hidden_size) \n",
    "            c0 = torch.zeros(num_layers, batch[0].shape[0], hidden_size)\n",
    "\n",
    "            # get ouput \n",
    "            outputs = rnn(batch[0], (h0, c0))\n",
    "\n",
    "            # reshape data for outputs#\n",
    "            outputs = outputs.reshape(-1, embedding_size)\n",
    "            targets = batch[1].reshape(-1)\n",
    "\n",
    "            # print(f\"output dim {outputs.size()}, target dims {targets.size()}\")\n",
    "          \n",
    "            # get loss \n",
    "            loss = obj_func(outputs, targets)\n",
    "            \n",
    "            # update network\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update batch loss\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch}:\\nBatch loss: {batch_loss}, normalized loss: {batch_loss/batch_size}\")\n",
    "        # store loss\n",
    "        e_loss[\"loss\"].append(batch_loss)\n",
    "        e_loss[\"norm_loss\"].append(batch_loss/batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Batch loss: 17.622108101844788, normalized loss: 1.9580120113160875\n",
      "Epoch 1:\n",
      "Batch loss: 4.604785189032555, normalized loss: 0.511642798781395\n",
      "Epoch 2:\n",
      "Batch loss: 2.312642350792885, normalized loss: 0.25696026119920945\n"
     ]
    }
   ],
   "source": [
    "# set hyperparameters\n",
    "embedding_size = 32\n",
    "hidden_size = 16\n",
    "epochs = 3\n",
    "num_classes = len(w2i)\n",
    "alpha = 0.03\n",
    "num_layers = 1\n",
    "\n",
    "train_autoreg(xtens_pb, ytens_pb, embedding_size, hidden_size, epochs, alpha, num_layers, opt = \"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequnceing strategy:\n",
    "\n",
    "xp_batch.append([1] + seq + [0]*(max_size - len(seq)) + [2])\n",
    "yp_batch.append( [0] + seq + [0]*(max_size - len(seq)) + [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Batch loss: 17.205260932445526, normalized loss: 1.911695659160614\n",
      "Epoch 1:\n",
      "Batch loss: 3.2505833879113197, normalized loss: 0.36117593199014664\n",
      "Epoch 2:\n",
      "Batch loss: 0.6622391305863857, normalized loss: 0.07358212562070952\n"
     ]
    }
   ],
   "source": [
    "# set hyperparameters\n",
    "embedding_size = 32\n",
    "hidden_size = 16\n",
    "epochs = 3\n",
    "num_classes = len(w2i)\n",
    "alpha = 0.03\n",
    "num_layers = 1\n",
    "\n",
    "train_autoreg(xtens_pb, ytens_pb, embedding_size, hidden_size, epochs, alpha, num_layers, opt = \"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequnceing strategy:\n",
    "\n",
    "xp_batch.append( seq + [0]*(max_size - len(seq)) + [2])\n",
    "yp_batch.append( [0] + seq[1:] + [0]*(max_size - len(seq)) + [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Batch loss: 17.33151751756668, normalized loss: 1.92572416861852\n",
      "Epoch 1:\n",
      "Batch loss: 2.9026977717876434, normalized loss: 0.3225219746430715\n",
      "Epoch 2:\n",
      "Batch loss: 0.5765323266386986, normalized loss: 0.06405914740429984\n"
     ]
    }
   ],
   "source": [
    "# set hyperparameters\n",
    "embedding_size = 32\n",
    "hidden_size = 16\n",
    "epochs = 3\n",
    "num_classes = len(w2i)\n",
    "alpha = 0.03\n",
    "num_layers = 1\n",
    "\n",
    "train_autoreg(xtens_pb, ytens_pb, embedding_size, hidden_size, epochs, alpha, num_layers, opt = \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial indeces:  [1, 2, 3, 4, 5, 6]\n",
      "Permute. num 1, indices = [2, 3, 4, 5, 6, 1]\n",
      "Permute. num 2, indices = [3, 4, 5, 6, 1, 2]\n",
      "Permute. num 3, indices = [4, 5, 6, 1, 2, 3]\n",
      "Permute. num 4, indices = [5, 6, 1, 2, 3, 4]\n",
      "Permute. num 5, indices = [6, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "index = [1, 2, 3, 4, 5, 6]\n",
    "def cycle_permute(index):\n",
    "    for i in range(len(index)- 2, 0, -1):\n",
    "        index = index[i:] + index[:i]\n",
    "    for i in range(1, len(index), 2):\n",
    "        index = index[i : ] + index[: i]\n",
    "    return index\n",
    "\n",
    "\n",
    "print(\"Initial indeces: \", index)\n",
    "for i in range(5):\n",
    "    index = cycle_permute(index)\n",
    "    print(f\"Permute. num {i +1}, indices = {index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(list):\n",
    "    converge = 1\n",
    "    while converge > 1e-6:\n",
    "\n",
    "        iterater = 1 if (list[int(len(list)/2)] * 2) / 3 < 1 else (list[int(len(list)/2)] * 2) / 3\n",
    "\n",
    "        for i in range(int(iterater)):\n",
    "            list = cycle_permute(list)\n",
    "            list = list[:i] + list[i:]\n",
    "        \n",
    "        converge /= list[int(len(list)/2)]\n",
    "    \n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial indeces:  [2, 3, 4, 5, 6, 1]\n",
      "Permute. num 1, indices = [5, 6, 1, 2, 3, 4]\n",
      "Permute. num 2, indices = [6, 1, 2, 3, 4, 5]\n",
      "Permute. num 3, indices = [2, 3, 4, 5, 6, 1]\n",
      "Permute. num 4, indices = [5, 6, 1, 2, 3, 4]\n",
      "Permute. num 5, indices = [6, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial indeces: \", index)\n",
    "for i in range(5):\n",
    "    index = shuffle(index)\n",
    "    print(f\"Permute. num {i +1}, indices = {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU for training.\n",
      "Epoch 1/10, Loss: 0.4731\n",
      "\n",
      "--- Samples after Epoch 1 ---\n",
      "Sample 1: suvw!uvw!uvw!uvw!s\n",
      "Sample 2: suvw!uvw!uvw!uvw!uvw!uvw!s\n",
      "Sample 3: sklm!klm!klm!klm!klm!s\n",
      "Sample 4: ss\n",
      "Sample 5: ss\n",
      "Sample 6: ss\n",
      "Sample 7: ss\n",
      "Sample 8: sklm!klm!klm!s\n",
      "Sample 9: ss\n",
      "Sample 10: ss\n",
      "-----------------------------------\n",
      "\n",
      "Epoch 2/10, Loss: 0.2458\n",
      "\n",
      "--- Samples after Epoch 2 ---\n",
      "Sample 1: ss\n",
      "Sample 2: sabc!abc!abc!abc!abc!s\n",
      "Sample 3: suvw!uvw!uvw!s\n",
      "Sample 4: sklm!klm!klm!klm!s\n",
      "Sample 5: sabc!abc!abc!s\n",
      "Sample 6: sklm!klm!klm!klm!klm!klm!klm!s\n",
      "Sample 7: sklm!klm!klm!s\n",
      "Sample 8: sklm!klm!s\n",
      "Sample 9: sabc!abc!s\n",
      "Sample 10: sklm!klm!s\n",
      "-----------------------------------\n",
      "\n",
      "Epoch 3/10, Loss: 0.2433\n",
      "\n",
      "--- Samples after Epoch 3 ---\n",
      "Sample 1: sabc!abc!abc!abc!abc!abc!abc!s\n",
      "Sample 2: suvw!uvw!uvw!uvw!uvw!uvw!s\n",
      "Sample 3: sklm!klm!klm!klm!klm!klm!s\n",
      "Sample 4: sabc!abc!abc!s\n",
      "Sample 5: sabc!s\n",
      "Sample 6: ss\n",
      "Sample 7: ss\n",
      "Sample 8: sklm!klm!klm!klm!klm!klm!s\n",
      "Sample 9: sabc!s\n",
      "Sample 10: sabc!abc!abc!abc!abc!abc!abc!abc!s\n",
      "-----------------------------------\n",
      "\n",
      "Epoch 4/10, Loss: 0.2427\n",
      "\n",
      "--- Samples after Epoch 4 ---\n",
      "Sample 1: ss\n",
      "Sample 2: sabc!s\n",
      "Sample 3: suvw!uvw!uvw!uvw!uvw!uvw!uvw!uvw!uvw!uvw!uvw!uvw!u\n",
      "Sample 4: sabc!s\n",
      "Sample 5: suvw!uvw!uvw!uvw!s\n",
      "Sample 6: suvw!s\n",
      "Sample 7: ss\n",
      "Sample 8: sklm!klm!klm!klm!klm!klm!klm!klm!s\n",
      "Sample 9: ss\n",
      "Sample 10: sklm!klm!klm!klm!klm!klm!s\n",
      "-----------------------------------\n",
      "\n",
      "Epoch 5/10, Loss: 0.2424\n",
      "\n",
      "--- Samples after Epoch 5 ---\n",
      "Sample 1: suvw!uvw!uvw!uvw!uvw!s\n",
      "Sample 2: sabc!s\n",
      "Sample 3: ss\n",
      "Sample 4: suvw!uvw!s\n",
      "Sample 5: suvw!uvw!uvw!uvw!uvw!s\n",
      "Sample 6: sklm!klm!s\n",
      "Sample 7: suvw!uvw!uvw!uvw!s\n",
      "Sample 8: ss\n",
      "Sample 9: sabc!abc!abc!abc!abc!abc!s\n",
      "Sample 10: suvw!uvw!uvw!uvw!uvw!s\n",
      "-----------------------------------\n",
      "\n",
      "Epoch 6/10, Loss: 0.2423\n",
      "\n",
      "--- Samples after Epoch 6 ---\n",
      "Sample 1: suvw!uvw!s\n",
      "Sample 2: ss\n",
      "Sample 3: sabc!abc!abc!abc!abc!s\n",
      "Sample 4: sklm!klm!klm!klm!s\n",
      "Sample 5: ss\n",
      "Sample 6: ss\n",
      "Sample 7: ss\n",
      "Sample 8: sabc!abc!abc!abc!abc!abc!abc!abc!abc!abc!abc!abc!s\n",
      "Sample 9: sklm!klm!klm!klm!s\n",
      "Sample 10: sabc!abc!abc!abc!abc!abc!abc!abc!abc!abc!s\n",
      "-----------------------------------\n",
      "\n",
      "Epoch 7/10, Loss: 0.2423\n",
      "\n",
      "--- Samples after Epoch 7 ---\n",
      "Sample 1: sklm!klm!klm!klm!klm!s\n",
      "Sample 2: suvw!s\n",
      "Sample 3: sklm!klm!klm!klm!klm!klm!klm!klm!s\n",
      "Sample 4: suvw!uvw!s\n",
      "Sample 5: ss\n",
      "Sample 6: suvw!s\n",
      "Sample 7: sabc!abc!abc!s\n",
      "Sample 8: suvw!uvw!uvw!uvw!uvw!uvw!uvw!uvw!uvw!uvw!s\n",
      "Sample 9: sklm!klm!s\n",
      "Sample 10: suvw!uvw!uvw!s\n",
      "-----------------------------------\n",
      "\n",
      "Epoch 8/10, Loss: 0.2424\n",
      "\n",
      "--- Samples after Epoch 8 ---\n",
      "Sample 1: ss\n",
      "Sample 2: suvw!uvw!uvw!uvw!uvw!uvw!s\n",
      "Sample 3: sabc!abc!abc!abc!abc!abc!s\n",
      "Sample 4: sklm!klm!klm!klm!klm!klm!klm!klm!klm!klm!klm!klm!k\n",
      "Sample 5: ss\n",
      "Sample 6: sklm!klm!klm!klm!s\n",
      "Sample 7: ss\n",
      "Sample 8: sabc!abc!s\n",
      "Sample 9: sabc!abc!abc!abc!abc!abc!abc!abc!s\n",
      "Sample 10: ss\n",
      "-----------------------------------\n",
      "\n",
      "Epoch 9/10, Loss: 0.2421\n",
      "\n",
      "--- Samples after Epoch 9 ---\n",
      "Sample 1: ss\n",
      "Sample 2: ss\n",
      "Sample 3: suvw!uvw!uvw!uvw!uvw!uvw!uvw!s\n",
      "Sample 4: sklm!klm!klm!klm!klm!klm!klm!klm!klm!s\n",
      "Sample 5: sabc!abc!abc!abc!s\n",
      "Sample 6: suvw!uvw!s\n",
      "Sample 7: sklm!klm!klm!klm!s\n",
      "Sample 8: ss\n",
      "Sample 9: ss\n",
      "Sample 10: ss\n",
      "-----------------------------------\n",
      "\n",
      "Epoch 10/10, Loss: 0.2423\n",
      "\n",
      "--- Samples after Epoch 10 ---\n",
      "Sample 1: sklm!klm!s\n",
      "Sample 2: sabc!abc!s\n",
      "Sample 3: suvw!uvw!s\n",
      "Sample 4: ss\n",
      "Sample 5: sabc!s\n",
      "Sample 6: sabc!abc!abc!abc!abc!abc!s\n",
      "Sample 7: sabc!abc!abc!abc!abc!abc!abc!abc!abc!abc!abc!abc!a\n",
      "Sample 8: sabc!s\n",
      "Sample 9: ss\n",
      "Sample 10: sklm!klm!klm!klm!klm!klm!klm!s\n",
      "-----------------------------------\n",
      "\n",
      "Validation Loss: 0.2422\n",
      "\n",
      "Final Sample: suvw!uvw!uvw!uvw!s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from data_rnn import *\n",
    "\n",
    "# ------------- Data Preparation -------------\n",
    "\n",
    "# Load the NDFA dataset\n",
    "x_train, (i2w, w2i) = load_ndfa(n=150_000, seed=0)\n",
    "\n",
    "# Define Hyperparameters\n",
    "embedding_dim = 32    # e=32\n",
    "hidden_size = 16      # h=16\n",
    "num_layers = 2        # Single layer\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "max_length = 50       # Adjust based on your data\n",
    "vocab_size = len(i2w) #15 for NDFA\n",
    "\n",
    "# Convert sequences to PyTorch tensors\n",
    "x_train_tensors = [torch.tensor(seq, dtype=torch.long) for seq in x_train]\n",
    "\n",
    "# Define maximum sequence length (based on your data or set a reasonable limit)\n",
    "max_length = 50  # Adjust as needed\n",
    "\n",
    "# Pad sequences with the `.pad` token (index 0) at the end\n",
    "x_train_padded = pad_sequence(\n",
    "    x_train_tensors,\n",
    "    batch_first=True,\n",
    "    padding_value=w2i['.pad']\n",
    ")\n",
    "\n",
    "# Truncate or pad to `max_length`\n",
    "if x_train_padded.size(1) > max_length:\n",
    "    x_train_padded = x_train_padded[:, :max_length]\n",
    "else:\n",
    "    padding = (0, max_length - x_train_padded.size(1))\n",
    "    x_train_padded = torch.nn.functional.pad(x_train_padded, padding, value=w2i['.pad'])\n",
    "\n",
    "# Creating Targets for Next-Token Prediction\n",
    "# Input: all tokens except the last\n",
    "# Target: all tokens except the first\n",
    "x_input = x_train_padded[:, :-1]  # Shape: (batch, time)\n",
    "y_target = x_train_padded[:, 1:]  # Shape: (batch, time)\n",
    "\n",
    "# Create TensorDataset\n",
    "dataset = TensorDataset(x_input, y_target)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# ------------- Model Definition -------------\n",
    "\n",
    "class SimpleLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
    "        super(SimpleLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=w2i['.pad']\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.LongTensor): Input tensor of shape (batch, time)\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: Output tensor of shape (batch, time, vocab_size)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # Shape: (batch, time, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embedded)  # Shape: (batch, time, hidden_size)\n",
    "        logits = self.fc(lstm_out)  # Shape: (batch, time, vocab_size)\n",
    "        return logits\n",
    "\n",
    "# ------------- Device Configuration -------------\n",
    "\n",
    "# Check for MPS (Apple GPU) support\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple GPU (MPS) for training.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA GPU for training.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU for training.\")\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleLSTMModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "# Move the model to the selected device\n",
    "model.to(device)\n",
    "\n",
    "# ------------- Loss Function and Optimizer -------------\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=w2i['.pad'])  # Ignore padding in loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ------------- Sampling Function -------------\n",
    "\n",
    "def generate_sequence(model, seed_seq, w2i, i2w, device, max_length=50):\n",
    "    \"\"\"\n",
    "    Generate a sequence by sampling from the model starting with a seed sequence.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained PyTorch model.\n",
    "        seed_seq (list of int): The seed sequence as a list of integer indices.\n",
    "        w2i (dict): Word-to-index mapping.\n",
    "        i2w (list): Index-to-word mapping.\n",
    "        device (torch.device): The device to run the model on.\n",
    "        max_length (int): Maximum length of the generated sequence.\n",
    "\n",
    "    Returns:\n",
    "        list of str: The generated sequence as a list of tokens.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated_seq = seed_seq.copy()\n",
    "    input_tensor = torch.tensor([generated_seq], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(seed_seq)):\n",
    "            outputs = model(input_tensor)\n",
    "            last_logits = outputs[0, -1, :]\n",
    "            probs = F.softmax(last_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated_seq.append(next_token)\n",
    "            if next_token == w2i['s']:\n",
    "                break\n",
    "            input_tensor = torch.tensor([generated_seq], dtype=torch.long).to(device)\n",
    "\n",
    "    generated_tokens = [i2w[idx] if idx < len(i2w) else '.unk' for idx in generated_seq]\n",
    "    return generated_tokens\n",
    "\n",
    "# ------------- Training Loop -------------\n",
    "\n",
    "# Define a seed sequence\n",
    "seed_sequence = [w2i['s']]  # Start with 's'\n",
    "\n",
    "# Number of samples to generate per epoch\n",
    "num_samples = 10\n",
    "\n",
    "# Training Loop\n",
    "model.train()  # Ensure the model is in training mode\n",
    "\n",
    "losses = []  # To track training loss\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_loss = 0\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        # Move data to the appropriate device\n",
    "        batch_x = batch_x.to(device)  # Shape: (batch, time)\n",
    "        batch_y = batch_y.to(device)  # Shape: (batch, time)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)  # Shape: (batch, time, vocab_size)\n",
    "\n",
    "        # Reshape outputs and targets for loss computation\n",
    "        outputs = outputs.view(-1, vocab_size)  # Shape: (batch * time, vocab_size)\n",
    "        batch_y = batch_y.view(-1)  # Shape: (batch * time)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f'Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Generate and print samples after each epoch, similar to the Q7 to check the model performance\n",
    "    print(f'\\n--- Samples after Epoch {epoch} ---')\n",
    "    for i in range(num_samples):\n",
    "        generated = generate_sequence(\n",
    "            model=model,\n",
    "            seed_seq=seed_sequence,\n",
    "            w2i=w2i,\n",
    "            i2w=i2w,\n",
    "            device=device,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        # Join tokens to form a string\n",
    "        generated_str = ''.join(generated)\n",
    "        print(f'Sample {i + 1}: {generated_str}')\n",
    "    print('-----------------------------------\\n')\n",
    "    # If you don't want to generate samples after per epoch, comment the part above\n",
    "\n",
    "# ------------- Evaluation -------------\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_x)\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            batch_y = batch_y.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "# Evaluate on the training data (or replace with validation DataLoader)\n",
    "val_loss = evaluate(model, dataloader, criterion, device)\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# ------------- Making Final Predictions -------------\n",
    "\n",
    "# Function to convert indices to tokens\n",
    "def indices_to_tokens(indices, i2w):\n",
    "    return [i2w[idx] if idx < len(i2w) else '.unk' for idx in indices]\n",
    "\n",
    "# Example prediction\n",
    "sample_sequence = [w2i['s']]  # Starting with 's'\n",
    "generated_tokens = generate_sequence(\n",
    "    model=model,\n",
    "    seed_seq=sample_sequence,\n",
    "    w2i=w2i,\n",
    "    i2w=i2w,\n",
    "    device=device,\n",
    "    max_length=max_length\n",
    ")\n",
    "# Join tokens to form a string\n",
    "generated_str = ''.join(generated_tokens)\n",
    "print(\"\\nFinal Sample:\", generated_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
