{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-06T19:56:30.184208Z",
     "start_time": "2024-12-06T19:56:30.182172Z"
    }
   },
   "source": [
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "import wget\n",
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T19:52:56.397137Z",
     "start_time": "2024-12-06T19:52:56.391966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ],
   "id": "15bf3efd173c27d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x109994170>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T19:52:40.856713Z",
     "start_time": "2024-12-06T19:52:40.851076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IMDB_URL = 'http://dlvu.github.io/data/imdb.{}.pkl.gz'\n",
    "IMDB_FILE = 'imdb.{}.pkl.gz'\n",
    "\n",
    "PAD, START, END, UNK = '.pad', '.start', '.end', '.unk'\n",
    "\n",
    "def load_imdb(final=False, val=5000, seed=0, voc=None, char=False):\n",
    "\n",
    "    cst = 'char' if char else 'word'\n",
    "\n",
    "    imdb_url = IMDB_URL.format(cst)\n",
    "    imdb_file = IMDB_FILE.format(cst)\n",
    "\n",
    "    if not os.path.exists(imdb_file):\n",
    "        wget.download(imdb_url)\n",
    "\n",
    "    with gzip.open(imdb_file) as file:\n",
    "        sequences, labels, i2w, w2i = pickle.load(file)\n",
    "\n",
    "    if voc is not None and voc < len(i2w):\n",
    "        nw_sequences = {}\n",
    "\n",
    "        i2w = i2w[:voc]\n",
    "        w2i = {w: i for i, w in enumerate(i2w)}\n",
    "\n",
    "        mx, unk = voc, w2i['.unk']\n",
    "        for key, seqs in sequences.items():\n",
    "            nw_sequences[key] = []\n",
    "            for seq in seqs:\n",
    "                seq = [s if s < mx else unk for s in seq]\n",
    "                nw_sequences[key].append(seq)\n",
    "\n",
    "        sequences = nw_sequences\n",
    "\n",
    "    if final:\n",
    "        return (sequences['train'], labels['train']), (sequences['test'], labels['test']), (i2w, w2i), 2\n",
    "\n",
    "    # Make a validation split\n",
    "    random.seed(seed)\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    x_val, y_val = [], []\n",
    "\n",
    "    val_ind = set( random.sample(range(len(sequences['train'])), k=val) )\n",
    "    for i, (s, l) in enumerate(zip(sequences['train'], labels['train'])):\n",
    "        if i in val_ind:\n",
    "            x_val.append(s)\n",
    "            y_val.append(l)\n",
    "        else:\n",
    "            x_train.append(s)\n",
    "            y_train.append(l)\n",
    "\n",
    "    return (x_train, y_train), \\\n",
    "           (x_val, y_val), \\\n",
    "           (i2w, w2i), 2"
   ],
   "id": "fde4e07092c71347",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T19:53:10.265166Z",
     "start_time": "2024-12-06T19:53:09.939844Z"
    }
   },
   "cell_type": "code",
   "source": "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = load_imdb(final=False)",
   "id": "cbf74f5602cc4a8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Samples: 20000\n",
      "Number of Validation Samples: 5000\n",
      "Vocabulary Size: 99430\n",
      "Number of Classes: 2\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T19:54:14.009892Z",
     "start_time": "2024-12-06T19:54:14.003397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_and_convert(sequences: List[List[int]], w2i: Dict[str, int],\n",
    "                   max_length: int = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pads a list of sequences to a fixed length and converts them to a PyTorch tensor.\n",
    "\n",
    "    Args:\n",
    "        sequences (List[List[int]]): A batch of sequences, where each sequence is a list of integer indices.\n",
    "        w2i (Dict[str, int]): A dictionary mapping words to their integer indices.\n",
    "        max_length (int, optional): The length to pad the sequences to. If None, uses the length of the longest sequence in the batch.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (batch_size, max_length) containing the padded sequences.\n",
    "    \"\"\"\n",
    "    # Retrieve the padding index from the w2i dictionary\n",
    "    pad_idx = w2i.get('.pad')\n",
    "    if pad_idx is None:\n",
    "        raise ValueError(\"The padding token '.pad' is not found in the w2i dictionary.\")\n",
    "\n",
    "    # Determine the maximum length for padding\n",
    "    if max_length is None:\n",
    "        max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "    # Initialize a list to hold the padded sequences\n",
    "    padded_sequences = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        # Calculate the number of padding tokens needed\n",
    "        padding_needed = max_length - len(seq)\n",
    "\n",
    "        if padding_needed < 0:\n",
    "            raise ValueError(\"A sequence is longer than the specified max_length.\")\n",
    "\n",
    "        # Pad the sequence with pad_idx\n",
    "        padded_seq = seq + [pad_idx] * padding_needed\n",
    "        padded_sequences.append(padded_seq)\n",
    "\n",
    "    # Convert the list of padded sequences to a PyTorch tensor with dtype torch.long\n",
    "    batch_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "    return batch_tensor\n",
    "\n",
    "def create_batches(sequences: List[List[int]], labels: List[int],\n",
    "                  batch_size: int, w2i: Dict[str, int]) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Splits the data into batches, pads each batch, and converts them to tensors.\n",
    "\n",
    "    Args:\n",
    "        sequences (List[List[int]]): List of all sequences.\n",
    "        labels (List[int]): Corresponding labels for each sequence.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        w2i (Dict[str, int]): Dictionary mapping words to their integer indices.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[torch.Tensor, torch.Tensor]]: A list of tuples, each containing padded sequences and their labels as tensors.\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    total_samples = len(sequences)\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size  # Ceiling division\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, total_samples)\n",
    "        batch_sequences = sequences[start_idx:end_idx]\n",
    "        batch_labels = labels[start_idx:end_idx]\n",
    "\n",
    "        # Pad and convert sequences\n",
    "        padded_sequences = pad_and_convert(batch_sequences, w2i)\n",
    "\n",
    "        # Convert labels to tensor\n",
    "        labels_tensor = torch.tensor(batch_labels, dtype=torch.long)\n",
    "\n",
    "        batches.append((padded_sequences, labels_tensor))\n",
    "\n",
    "    return batches"
   ],
   "id": "dc30a572cb1194ca",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T19:54:56.100381Z",
     "start_time": "2024-12-06T19:54:55.903118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "train_batches = create_batches(x_train, y_train, batch_size, w2i)\n",
    "val_batches = create_batches(x_val, y_val, batch_size, w2i)"
   ],
   "id": "be56c7335ff02557",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T19:58:06.110462Z",
     "start_time": "2024-12-06T19:58:06.108381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"mps\")\n",
    "print(f'Using device: {device}')"
   ],
   "id": "8f68156db9a3d844",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T20:00:19.838089Z",
     "start_time": "2024-12-06T20:00:19.834069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 300, hidden_size: int = 300, num_classes: int = 2):\n",
    "        \"\"\"\n",
    "        Initializes the Sequence-to-Sequence Model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): Number of unique tokens in the vocabulary.\n",
    "            embedding_dim (int, optional): Dimension of the embedding vectors. Defaults to 300.\n",
    "            hidden_size (int, optional): Dimension of the hidden layer. Defaults to 300.\n",
    "            num_classes (int, optional): Number of output classes. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        \n",
    "        # 1) Embedding layer: Converts integer indices to embedding vectors\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # 2) Linear layer: Maps each embedding vector to a hidden representation\n",
    "        self.linear = nn.Linear(in_features=embedding_dim, out_features=hidden_size)\n",
    "        \n",
    "        # 3) ReLU activation: Introduces non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 5) Final Linear layer: Projects the pooled representation to the number of classes\n",
    "        self.output_linear = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, time_steps), dtype=torch.long\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, num_classes), dtype=torch.float\n",
    "        \"\"\"\n",
    "        # 1) Embedding: (batch, time) -> (batch, time, embedding_dim)\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        # 2) Linear layer: (batch, time, embedding_dim) -> (batch, time, hidden_size)\n",
    "        linear_out = self.linear(embeds)\n",
    "        \n",
    "        # 3) ReLU activation: (batch, time, hidden_size) -> (batch, time, hidden_size)\n",
    "        relu_out = self.relu(linear_out)\n",
    "        \n",
    "        # 4) Global max pool along the time dimension: (batch, time, hidden_size) -> (batch, hidden_size)\n",
    "        # torch.max returns a tuple (values, indices). We take the first element (values).\n",
    "        pooled_out, _ = torch.max(relu_out, dim=1)\n",
    "        \n",
    "        # 5) Final Linear layer: (batch, hidden_size) -> (batch, num_classes)\n",
    "        output = self.output_linear(pooled_out)\n",
    "        \n",
    "        # 6) Output tensor: (batch, num_classes)\n",
    "        return output"
   ],
   "id": "adb78fe612cb7fb8",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T20:01:21.947260Z",
     "start_time": "2024-12-06T20:01:21.630739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Initialization\n",
    "vocab_size = len(i2w)  # Vocabulary size from load_imdb\n",
    "embedding_dim = 300\n",
    "hidden_size = 300\n",
    "num_classes = 2\n",
    "pad_idx = w2i.get('.pad', 0)  # Default to 0 if '.pad' not found\n",
    "\n",
    "model = Seq2SeqModel(vocab_size=vocab_size,\n",
    "                           embedding_dim=embedding_dim,\n",
    "                           hidden_size=hidden_size,\n",
    "                           num_classes=num_classes).to(device)\n",
    "\n",
    "print(model)"
   ],
   "id": "2acae3f95f9a5dc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqModel(\n",
      "  (embedding): Embedding(99430, 300)\n",
      "  (linear): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (output_linear): Linear(in_features=300, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T20:02:39.752077Z",
     "start_time": "2024-12-06T20:02:39.252244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Accuracy Calculation\n",
    "def calculate_accuracy(preds, labels):\n",
    "    _, predicted = torch.max(preds, dim=1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    accuracy = correct / labels.size(0) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Epoch Time Calculation\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ],
   "id": "e0cbc5accdfc478b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T20:04:16.625241Z",
     "start_time": "2024-12-06T20:04:16.619274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training Function\n",
    "def train_model(model, train_batches, val_batches, criterion, optimizer, device, num_epochs=10):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, (batch_sequences, batch_labels) in enumerate(train_batches):\n",
    "            # Move data to device\n",
    "            batch_sequences = batch_sequences.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_sequences)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += calculate_accuracy(outputs, batch_labels)\n",
    "\n",
    "            if (batch_idx + 1) % 50 == 0 or (batch_idx + 1) == len(train_batches):\n",
    "                print(f'Epoch [{epoch}/{num_epochs}], Batch [{batch_idx + 1}/{len(train_batches)}], '\n",
    "                      f'Loss: {loss.item():.4f}, Accuracy: {calculate_accuracy(outputs, batch_labels):.2f}%')\n",
    "\n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        avg_loss = epoch_loss / len(train_batches)\n",
    "        avg_acc = epoch_acc / len(train_batches)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_sequences, batch_labels in val_batches:\n",
    "                # Move data to device\n",
    "                batch_sequences = batch_sequences.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(batch_sequences)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                val_acc += calculate_accuracy(outputs, batch_labels)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_batches)\n",
    "        avg_val_acc = val_acc / len(val_batches)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch [{epoch}/{num_epochs}] completed in {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'Training Loss: {avg_loss:.4f}, Training Accuracy: {avg_acc:.2f}%')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_acc:.2f}%\\n')\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_simple_seq2seq_model.pth')\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break"
   ],
   "id": "b300ae17ddab8544",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T20:05:19.396643Z",
     "start_time": "2024-12-06T20:04:47.731041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Start Training\n",
    "num_epochs = 2\n",
    "train_model(model, train_batches, val_batches, criterion, optimizer, device, num_epochs)\n",
    "\n",
    "# Load the Best Model (Optional)\n",
    "best_model = Seq2SeqModel(vocab_size=vocab_size,\n",
    "                                embedding_dim=embedding_dim,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_classes=num_classes).to(device)\n",
    "best_model.load_state_dict(torch.load('best_simple_seq2seq_model.pth'))\n",
    "print('Best model loaded for evaluation.')"
   ],
   "id": "bcdec2eaba2fafff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Batch [50/313], Loss: 0.4294, Accuracy: 79.69%\n",
      "Epoch [1/2], Batch [100/313], Loss: 0.4173, Accuracy: 81.25%\n",
      "Epoch [1/2], Batch [150/313], Loss: 0.4022, Accuracy: 81.25%\n",
      "Epoch [1/2], Batch [200/313], Loss: 0.2980, Accuracy: 85.94%\n",
      "Epoch [1/2], Batch [250/313], Loss: 0.4032, Accuracy: 81.25%\n",
      "Epoch [1/2], Batch [300/313], Loss: 0.4539, Accuracy: 82.81%\n",
      "Epoch [1/2], Batch [313/313], Loss: 0.2485, Accuracy: 90.62%\n",
      "Epoch [1/2] completed in 0m 21s\n",
      "Training Loss: 0.3876, Training Accuracy: 81.90%\n",
      "Validation Loss: 0.3227, Validation Accuracy: 86.37%\n",
      "\n",
      "Epoch [2/2], Batch [50/313], Loss: 0.1777, Accuracy: 92.19%\n",
      "Epoch [2/2], Batch [100/313], Loss: 0.2695, Accuracy: 89.06%\n",
      "Epoch [2/2], Batch [150/313], Loss: 0.2708, Accuracy: 89.06%\n",
      "Epoch [2/2], Batch [200/313], Loss: 0.1761, Accuracy: 95.31%\n",
      "Epoch [2/2], Batch [250/313], Loss: 0.2935, Accuracy: 87.50%\n",
      "Epoch [2/2], Batch [300/313], Loss: 0.3206, Accuracy: 87.50%\n",
      "Epoch [2/2], Batch [313/313], Loss: 0.1509, Accuracy: 90.62%\n",
      "Epoch [2/2] completed in 0m 9s\n",
      "Training Loss: 0.2379, Training Accuracy: 90.34%\n",
      "Validation Loss: 0.2956, Validation Accuracy: 88.15%\n",
      "\n",
      "Best model loaded for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/37/2gmnw4j537174_8_k_lgdcch0000gn/T/ipykernel_4250/3552683735.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load('best_simple_seq2seq_model.pth'))\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T20:14:16.661089Z",
     "start_time": "2024-12-06T20:14:16.658212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, test_batches, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained sequence model.\n",
    "        test_batches (List[Tuple[torch.Tensor, torch.Tensor]]): Test data batches.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to run the evaluation on.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_sequences, batch_labels in test_batches:\n",
    "            # Move data to the appropriate device\n",
    "            batch_sequences = batch_sequences.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_sequences)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            test_acc += calculate_accuracy(outputs, batch_labels)\n",
    "    \n",
    "    # Calculate average loss and accuracy\n",
    "    avg_test_loss = test_loss / len(test_batches)\n",
    "    avg_test_acc = test_acc / len(test_batches)\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {avg_test_acc:.2f}%')"
   ],
   "id": "89aa250cbf0647a0",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T20:14:25.628415Z",
     "start_time": "2024-12-06T20:14:21.969699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Test Batches\n",
    "# For example purposes, let's assume you have a function similar to create_batches\n",
    "(x_test, y_test), _, _, _ = load_imdb(final= True)  # Modify as needed\n",
    "test_batches = create_batches(x_test, y_test, batch_size, w2i)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "evaluate_model(best_model, test_batches, criterion, device)"
   ],
   "id": "be448920f1a0d129",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1946, Test Accuracy: 92.56%\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
